{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "DATA_FOLDER = '../data/Cleantech Media Dataset'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_csv(f'{DATA_FOLDER}/cleantech_media_dataset_v2_2024-02-23.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "columns = df.columns\n",
    "total_counts = df.count()\n",
    "nan_counts = df.isna().sum()\n",
    "unique_counts = df.nunique()\n",
    "\n",
    "nan_percentages = (nan_counts / len(df)) * 100\n",
    "unique_percentages = (unique_counts / len(df)) * 100\n",
    "\n",
    "data = {\n",
    "    'Total Count': total_counts,\n",
    "    'NaN Count': nan_counts,\n",
    "    'NaN Percentage (%)': nan_percentages,\n",
    "    'Unique Count': unique_counts,\n",
    "    'Unique Percentage (%)': unique_percentages\n",
    "}\n",
    "summary_df = pd.DataFrame(data, index=columns)\n",
    "\n",
    "summary_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "domain_freq = df['domain'].value_counts()\n",
    "domain_freq = domain_freq.reset_index()\n",
    "domain_freq.columns = ['domain', 'count']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=domain_freq['domain'], y=domain_freq['count']))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Frequency of Publishers in Cleantech',\n",
    "    xaxis_title='Domain',\n",
    "    yaxis_title='Frequency'\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at titles\n",
    "As the summary has shown, only `9569` of the `9593` scraped resources in the dataset have a unique title. This subsection explores if these \"duplicate titles\" have an underlying error or if these occurences of duplicates can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "title_freq = df['title'].value_counts()\n",
    "title_freq = title_freq[title_freq > 1]\n",
    "title_freq = title_freq.reset_index()\n",
    "title_freq.columns = ['title', 'count']\n",
    "\n",
    "title_freq"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets take a closer look at the contents of the suspected duplicate documents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_all_duplicate_document_contents(df, title_freq):\n",
    "    duplicates_counts = {}\n",
    "\n",
    "    for title in title_freq['title']:\n",
    "        duplicate_contents = df[df['title'] == title]['content']\n",
    "        duplicate_contents = duplicate_contents.apply(ast.literal_eval)\n",
    "        duplicate_contents = duplicate_contents.explode()\n",
    "        duplicates_count = duplicate_contents.duplicated().sum()\n",
    "        duplicates_counts[title] = duplicates_count\n",
    "\n",
    "    return pd.DataFrame(list(duplicates_counts.items()), columns=['title', 'duplicated_count'])\n",
    "\n",
    "\n",
    "duplicated_title_contents = calculate_all_duplicate_document_contents(df, title_freq)\n",
    "\n",
    "duplicated_title_contents"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function yielded that the duplicate observations of title contain actual duplicate information on chunk-basis.\n",
    "\n",
    "This could mean that there are even more duplicate chunks under titles that aren't duplicate, so lets next look at that:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['content'] = df['content'].apply(ast.literal_eval)\n",
    "df_exploded_contents = df.explode('content')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'Total duplicated contents: {df_exploded_contents.duplicated().sum()}'\n",
    "      f'\\nTotal duplicated contents from duplicated titles: {duplicated_title_contents[\"duplicated_count\"].sum()}'\n",
    "      f'\\nTotal duplicated contents from non-duplicated titles: {df_exploded_contents.duplicated().sum() - duplicated_title_contents[\"duplicated_count\"].sum()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shows, another `629` chunks on top of the `264` duplicates inside the duplicate-title-occurences emerged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Languages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def safe_detect(text):\n",
    "    text = str(text)\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "df['language'] = df['content'].apply(safe_detect)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['language'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lang_anomlaies = df[(df['language'] == 'ru') | (df['language'] == 'de')]['Unnamed: 0'].to_list()\n",
    "lang_anomlaies"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "non_english_chunks = []\n",
    "\n",
    "for lang_anomaly in lang_anomlaies:\n",
    "    content = df[df['Unnamed: 0'] == lang_anomaly]['content']\n",
    "    for chunk in range(len(content.values[0])):\n",
    "        if detect(content.values[0][chunk]) != 'en':\n",
    "            non_english_chunks.append((lang_anomaly, chunk))\n",
    "\n",
    "non_english_chunks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.preprocessing import Preprocessor\n",
    "\n",
    "pp = Preprocessor(dataframe=df, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pp.preprocess()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pp.df.head()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
