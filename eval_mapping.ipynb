{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Map Evaluation Data to Original Dataset\n",
    "\n",
    "As part of the evaluation of our RAG system we need to map the relevant chunks of the evaluation set to the original dataset. This is necessary to be able to evaluate the performance of the RAG system with metrics such as Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP). In this notebook we will assess the matching strategies of relevant chunk in the evaluation set to the original dataset."
   ],
   "id": "795cc67d18b1664"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "\n",
    "First the data is loaded."
   ],
   "id": "a96ca314ae7b4576"
  },
  {
   "cell_type": "code",
   "id": "d028a391597bb1de",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23.csv')\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_eval_subset = pd.read_csv('data/Cleantech Media Dataset/cleantech_rag_evaluation_data_2024-02-23.csv')\n",
    "df_eval_subset = df_eval_subset.drop_duplicates().sample(10)\n",
    "df_eval_subset.head()"
   ],
   "id": "bd504301645fdf53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess Data\n",
    "\n",
    "In order to be able to associate each chunk with a specific document each document needs to have an unique identifier. The creation of an unique identifier is done in the preprocessing class as we take the content of the document and hash it. This hash is then used as the unique identifier for the document. The preprocessing class also takes care of other preprocessing steps such as removing duplicates and concatenating the content of the document, more information can be found in the [preprocessing notebook](preprocessing.ipynb)."
   ],
   "id": "15ccdb8ee62c330c"
  },
  {
   "cell_type": "code",
   "id": "1d6d6b974dc56bb8",
   "metadata": {},
   "source": [
    "from src.preprocessing.preprocessor import Preprocessor\n",
    "\n",
    "default_df = Preprocessor(df, verbose=True, explode=False, concatenate_contents=True).preprocess()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After preprocessing the dataframe has the new column `id` which is the unique identifier for each document.",
   "id": "8093604cc7f7ed0e"
  },
  {
   "cell_type": "code",
   "id": "ad1d73f0cad6c2a5",
   "metadata": {
    "collapsed": false
   },
   "source": "default_df['id'].duplicated().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mapping Evaluation Data to Original Dataset\n",
    "\n",
    "The mapping between relevant chunks from the evaluation set and documents in the dataset is done in the EvaluationSetPreprocessor class. this class uses a fuzzy matching strategy to find the best match for each relevant chunk in the dataset. The best match is then stored in the `best_match_id` column along with the `best_match_score` which is the similarity score between the relevant chunk and the best match."
   ],
   "id": "4a9d2b31f81a9580"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.preprocessing.eval_preprocessor import EvaluationSetPreprocessor\n",
    "\n",
    "eval_processor = EvaluationSetPreprocessor(default_df, df_eval_subset, verbose=True)\n",
    "eval_df = eval_processor.preprocess()\n",
    "eval_df.head()"
   ],
   "id": "d5682144627c5961",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Assess Matching Strategy\n",
    "\n",
    "Now we humanly assess the matching strategy by highlighting the relevant chunk and the best match in the original dataset. The `highlight_matches` function takes the evaluation dataframe and the original dataframe as input and highlights the relevant chunk and the best match in the original dataset. The `min_words` parameter can be used to specify the minimum number of words that should be highlighted. It does not work perfectly but it gives a good idea of how well the matching strategy is working helping us to find the relevant chunks in the original document content."
   ],
   "id": "9ba455398b583726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def highlight_matches(eval_df: pd.DataFrame, default_df: pd.DataFrame, min_words=2):\n",
    "    def highlight_text(text, match, min_words):\n",
    "        words_pattern = r'\\b(' + '|'.join(re.escape(word) for word in match.split()) + r')\\b'\n",
    "        regex = rf\"({words_pattern}(?:\\s+{words_pattern})*)\"\n",
    "\n",
    "        def highlighter(match):\n",
    "            word_count = len(match.group(0).split())\n",
    "            if word_count >= min_words:\n",
    "                return f\"<mark>{match.group(0)}</mark>\"\n",
    "            else:\n",
    "                return match.group(0)\n",
    "\n",
    "        highlighted = re.sub(regex, highlighter, text, flags=re.IGNORECASE)\n",
    "        return highlighted\n",
    "\n",
    "    for index, row in eval_df[:2].iterrows():\n",
    "        best_match = default_df[default_df['id'] == row['best_match_id']]\n",
    "\n",
    "        if not best_match.empty:\n",
    "            best_match_content = best_match['content'].values[0]\n",
    "            highlighted_content = highlight_text(best_match_content, row['relevant_chunk'], min_words)\n",
    "        else:\n",
    "            highlighted_content = \"No match found\"\n",
    "\n",
    "        display(HTML(f\"<b>Relevant chunk:</b> {row['relevant_chunk']}\"))\n",
    "        print(\"\\n\")\n",
    "        display(HTML(f\"<b>Best Match:</b> {highlighted_content}\"))\n",
    "        print(\"\\n\\n\" * 2)\n",
    "\n",
    "\n",
    "highlight_matches(eval_df, default_df)"
   ],
   "id": "a4ce9d9cf7160b46",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
