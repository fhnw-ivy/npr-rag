{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393b298e",
   "metadata": {},
   "source": [
    "# npr MC1: Cleantech Retrieval Augemented Generation\n",
    "\n",
    "**Dominik Filliger, Nils Fahrni, Noah Leuenberger**\n",
    "\n",
    "> The topic of Mini-Challenge 1 is retrieval augmented generation (RAG) incorporating a combination of unsupervised learning, pre-training and in-context learning techniques.\n",
    "\n",
    "- [Description of the task](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/exercises/NPR-Mini-Challenge-1-Cleantech-RAG-1708982891.pdf)\n",
    "- [Introduction to RAG](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/exercises/Retrieval-Augmented-Generation-Intro-1709021241.pdf)\n",
    "\n",
    "This notebook serves as the main entry point for our solution to the NPR Mini-Challenge 1. We will provide a detailed explanation of our approach and the code we used to solve the task. However, we have outsourced the code for the evaluation, Langchain LLM model creation and vectorstore interaction to script files which can be found in the `src` directory.\n",
    "\n",
    "Additionally, scripts for the development subset and subset evaluation set creation can be found in the `scripts` directory and will be referenced in their respective sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c7169756adcbe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "188c966d5cd28de1",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from src.generation import get_llm_model, LLMModel\n",
    "\n",
    "azure_model = get_llm_model(LLMModel.GPT_3_AZURE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47adf37081d73c21",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Observability & Monitoring\n",
    "\n",
    "> Phoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n",
    "\n",
    "We will use Phoenix to visualize traces to quickly debug pipelines. The library offers way more feature which we will not use. Down below we add the Phoenix callbacks to Langchain, our main library for the solution, to visualize the traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d0092013e36ca8c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "import phoenix as px\n",
    "\n",
    "px.close_app()\n",
    "session = px.launch_app()\n",
    "\n",
    "LangChainInstrumentor().instrument()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "caa766a0",
   "metadata": {},
   "source": [
    "To get quick access to the Phoenix dashboard, the dashboard is rendered in the notebook. The dashboard is interactive and can be used to explore the traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "abd676a30f7effbe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "session.view()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f9f2033ded77748",
   "metadata": {
    "collapsed": false
   },
   "source": "# Data Loading & Preprocessing\n"
  },
  {
   "cell_type": "code",
   "id": "d028a391597bb1de",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23.csv')\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d6d6b974dc56bb8",
   "metadata": {},
   "source": [
    "from src.preprocessing.preprocessor import Preprocessor\n",
    "\n",
    "preprocessed_df_path = 'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23_preprocessed.csv'\n",
    "if os.path.exists(preprocessed_df_path):\n",
    "    preprocessed_df = pd.read_csv(preprocessed_df_path)\n",
    "else:\n",
    "    preprocessed_df = Preprocessor(df).preprocess()\n",
    "    preprocessed_df.to_csv(preprocessed_df_path, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29f172af1ea76470",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Indexing\n",
    "\n",
    "The indexing involves creating a vector representation of the content.\n",
    "\n",
    "The chosen embedding model is the BGE-Small-EN model from HuggingFace. The model is a transformer-based model which is trained on the BGE dataset. The model is used to create embeddings for the content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "id": "af0a4a16684d3b86",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f066402773ac9d58",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "\n",
    "def get_document_metadata(row):\n",
    "    return {\n",
    "        \"url\": row['url'],\n",
    "        \"domain\": row['domain'],\n",
    "        \"title\": row['title'],\n",
    "        \"author\": row['author'],\n",
    "        \"date\": row['date'],\n",
    "        \"origin_doc_id\": row['id']\n",
    "    }\n",
    "\n",
    "\n",
    "documents = [Document(page_content=row['content'], metadata=get_document_metadata(row))\n",
    "             for index, row in preprocessed_df.iterrows()\n",
    "             for split in recursive_text_splitter.split_text(row['content'])]\n",
    "\n",
    "print(f\"Number of documents: {len(documents)}, Number of rows in df: {len(preprocessed_df)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec5d15f4",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "\n",
    "We will use [ChromaDB](https://www.trychroma.com/) to store the embeddings. For easier interaction with the embeddings, we will use the VectorStore class which is a wrapper around the embeddings and ChromaDB. It provides a simple interface to interact with the embeddings and ChromaDB functionality we need for the task.\n",
    "\n",
    "### ChromaDB Setup\n",
    "\n",
    "If the environment variables `CHROMADB_HOST` or `CHROMADB_PORT` are not set, the VectorStore will use a local non-persistent ChromaDB client, which is not recommended. Instead we recommend setting up a ChromaDB instance. The ChromaDB instance can be set up using the following command and Docker:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d chromadb\n",
    "```\n",
    "\n",
    "Set the environment variables `CHROMADB_HOST` and `CHROMADB_PORT` to the host and port of the ChromaDB instance. The default values are `localhost` and `8192`.\n",
    "\n",
    "### VectorStore Usage\n",
    "\n",
    "The vector store is directly tied to the embeddings. Therefore a vector store is embedding specific and can only be used with the embeddings it was created with."
   ]
  },
  {
   "cell_type": "code",
   "id": "f738ace88dd27a69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.vectorstore import VectorStore\n",
    "\n",
    "print(\"ChromeDB Host: \", os.getenv('CHROMADB_HOST'))\n",
    "print(\"ChromeDB Port: \", os.getenv('CHROMADB_PORT'))\n",
    "print(\"ChromaDB Collection: \", os.getenv('CHROMADB_COLLECTION'))\n",
    "\n",
    "bge_vector_store = VectorStore(embedding_function=bge_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f20213c70591f14",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the next step we will add the prepared documents from the previous step to the VectorStore."
   ]
  },
  {
   "cell_type": "code",
   "id": "62d558939648a829",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%script false --no-raise-error\n",
    "bge_vector_store.add_documents(documents, verbose=True, batch_size=128)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2177cd39686a8db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After adding the documents to the vector store we can now perform similarity searches on the documents to verify that the interaction with the vector store works as expected."
   ]
  },
  {
   "cell_type": "code",
   "id": "91a13de926826f7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bge_vector_store.similarity_search_w_scores(\"The company is also aiming to reduce gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ad87a39c366b0b3",
   "metadata": {},
   "source": [
    "# Baseline Pipeline\n",
    "\n",
    "The baseline pipeline is a first simple implementation of the RAG pipeline.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "retriever = bge_vector_store.get_retriever()"
   ],
   "id": "7bdd2bf51dc298ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcc6ebdf4dab2924",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "base_rag_prompt = \"\"\"\n",
    "Answer the question to your best knowledge when looking at the following context:\n",
    "{context}\n",
    "                \n",
    "Question: {question}\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83dfe841ed24a7a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "base_rag_chain = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | ChatPromptTemplate.from_template(base_rag_prompt)\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "base_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8b1225784152974",
   "metadata": {
    "collapsed": false
   },
   "source": "base_rag.invoke(\"Is the company aiming to reduce gas flaring?\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a0c9f4f51cdcb2f",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In order to compare the performance of different pipelines we need to evaluate them. The evaluation is done with the `ragas` library. The library provides a function to evaluate the performance of the pipeline. `ragas` provides predefined metrics for the evaluation which are described in the [documentation](https://docs.ragas.io/en/stable/concepts/metrics/index.html). We will use the following metrics to evaluate the performance of our pipelines:\n",
    "\n",
    "- **Context Relevancy**: The context relevancy metric measures how well the generated response is related to the context. The metric is calculated as the cosine similarity between the context and the generated response.\n",
    "\n",
    "## Evaluation Set\n",
    "In order to provide a fair comparison between the different pipelines we will use the same evaluation set for all pipelines. The evaluation set was created before hand with the script `scripts/generate_testset.py`. With that we can evaluate the performance of our pipelines with a subset of the data which saves time and resources."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad1d73f0cad6c2a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_eval_subset = pd.read_csv('data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23_subset_eval.csv')\n",
    "df_eval_subset = df_eval_subset.dropna(subset=['answer'])\n",
    "df_eval_subset = df_eval_subset.drop_duplicates().sample(10)\n",
    "df_eval_subset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ae15782",
   "metadata": {},
   "source": [
    "## RAGEvaluator\n",
    "The RAGEvaluator evaluation class is a wrapper around the `ragas` library. It provides a simple interface to evaluate the performance of the pipelines. The class provides a method to evaluate the performance of the pipeline and returns the results as a pandas DataFrame. The metrics are calculated for each example in the evaluation set and results can be aggregated over the whole evaluation set to get an overall performance of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5bccbe256adad5d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.evaluation import RAGEvaluator\n",
    "\n",
    "base_evaluator = RAGEvaluator(name=\"Baseline\",\n",
    "                              chain=base_rag,\n",
    "                              llm_model=azure_model,\n",
    "                              embeddings=bge_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f11f14347ce18bb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "base_evaluator.create_dataset_from_df(df_eval_subset)\n",
    "default_eval_result = base_evaluator.evaluate(raise_exceptions=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "base_evaluator.summarize_metrics()",
   "metadata": {
    "collapsed": false
   },
   "id": "835890b412a2afca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8641b16",
   "metadata": {},
   "source": [
    "# Experiment 1: Looking at the impact of context and its chunking strategy\n",
    "\n",
    "Contrary to the apparent structure of the data, which seems to have already chunked the data according to the first proposed chunking strategy, this step will introduce the concatenation of these premade chunks into one single document. This will help us to see if it is beneficial for the LLM to have the entire document context instead of just a chunk of said document.\n",
    "\n",
    "First, to restructure the cleantech-dataset's content structure, we can call the `preprocess()` method on the `Preprocessor` Object which was instantiated using the `concatenate_contents=True` attribute. This will turn the list of all prechunked contents into a joined string, representing the content of every document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d18816",
   "metadata": {},
   "source": [
    "In order to embed the processed documents we again can turn them into langchain-digestible Documents."
   ]
  },
  {
   "cell_type": "code",
   "id": "18b55e88",
   "metadata": {},
   "source": [
    "full_content_documents = [Document(page_content=row['content'], metadata=get_document_metadata(row))\n",
    "                          for _, row in preprocessed_df.iterrows()]\n",
    "\n",
    "assert len(full_content_documents) == len(preprocessed_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4edca8df",
   "metadata": {},
   "source": [
    "And in order to look at this experiment in an encapsulated manner, a new `VectorStore` will be created."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad572672",
   "metadata": {},
   "source": [
    "bge_full_content_vector_store = VectorStore(embedding_function=bge_embeddings,\n",
    "                                            collection=\"cleantech-full-content-bge-small-en\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "full_retriever = bge_full_content_vector_store.get_retriever()",
   "id": "7f2bba0e669126d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15eb78af",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "bge_full_content_vector_store.add_documents(full_content_documents, verbose=True, batch_size=128)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8cac4a3",
   "metadata": {},
   "source": [
    "bge_full_content_vector_store.similarity_search_w_scores(\"The company is also aiming to reduce gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03a1d3e3",
   "metadata": {},
   "source": [
    "full_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": full_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f76fca6",
   "metadata": {},
   "source": "full_rag.invoke(\"Is the company aiming to reduce gas flaring?\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40e01e49",
   "metadata": {},
   "source": [
    "full_evaluator = RAGEvaluator(name=\"Full Content\",\n",
    "                              chain=full_rag,\n",
    "                              llm_model=azure_model,\n",
    "                              embeddings=bge_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe40b165",
   "metadata": {},
   "source": [
    "full_evaluator.create_dataset_from_df(df_eval_subset)\n",
    "full_content_eval_results = full_evaluator.evaluate(raise_exceptions=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment 2: Using a Multi-Query Retrieval Strategy\n",
    "\n",
    "At the heart of the RAG is the retriever, which is responsible for finding the most relevant documents for a given question. The baseline RAG uses the vector retriever to find the most relevant document, using cosine-similarity. \n",
    "\n",
    "We will now experiment with a multi-query retrieval strategy. The idea is to use multiple queries to retrieve a multidude of documents and take a unique union of the results. This way we can increase the diversity of the documents and potentially improve the quality of the generated answer. \n",
    "\n",
    "For this we will use the MultiQueryRetriever from langchain.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4206849bf946549"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83848be6543bbf39",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mqr_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=azure_model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb34e7a5e5ea5fc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "## using the langchain template for the prompt\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "        prompt_perspectives\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569145c02e8b6f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "# Retrieve\n",
    "mqr_retrieval_chain = (\n",
    "        generate_queries\n",
    "        | mqr_retriever.map()\n",
    "        | get_unique_union\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeeaa115ecd44569",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mqr_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": mqr_retrieval_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6515b7c5f45f3e2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mqr_rag.invoke(\"Is the company aiming to reduce gas flaring?\")",
   "metadata": {
    "collapsed": false
   },
   "id": "4ea2b6ac1c956550",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mqr_evaluator = RAGEvaluator(name=\"Multi-Query Retrieval\",\n",
    "                             chain=mqr_rag,\n",
    "                             llm_model=azure_model,\n",
    "                             embeddings=bge_embeddings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaece981dfdcbbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mqr_evaluator.create_dataset_from_df(df_eval_subset)\n",
    "mqr_eval_results = mqr_evaluator.evaluate(raise_exceptions=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cc9986273960c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mqr_evaluator.summarize_metrics()",
   "metadata": {
    "collapsed": false
   },
   "id": "649b8d0c0c489ba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "base_evaluator.summarize_metrics()",
   "metadata": {
    "collapsed": false
   },
   "id": "7545409b079eb030",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "full_evaluator.summarize_metrics()",
   "metadata": {
    "collapsed": false
   },
   "id": "66d204453dc7871c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
