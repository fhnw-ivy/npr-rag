{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6750e249",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [npr MC1: Cleantech Retrieval Augemented Generation](#toc1_)    \n",
    "- [Structure of our Solution](#toc2_)    \n",
    "- [Setup](#toc3_)    \n",
    "  - [Pathing](#toc3_1_)    \n",
    "  - [Observability & Monitoring](#toc3_2_)    \n",
    "- [Data Loading & Preprocessing](#toc4_)    \n",
    "- [Indexing](#toc5_)    \n",
    "  - [Chunking](#toc5_1_)    \n",
    "  - [Embeddings](#toc5_2_)    \n",
    "  - [Vector Store](#toc5_3_)    \n",
    "- [Baseline Pipeline](#toc6_)    \n",
    "- [Evaluation Setup](#toc7_)    \n",
    "  - [RAGAS Metrics](#toc7_1_)    \n",
    "  - [Non-LLM Based Metrics](#toc7_2_)    \n",
    "  - [Evaluation Data](#toc7_3_)    \n",
    "    - [Plausibility Checks](#toc7_3_1_)    \n",
    "- [Helper Functions](#toc8_)    \n",
    "- [Experiment 1: Looking at the impact of context and its chunking strategy](#toc9_)    \n",
    "    - [Results](#toc9_1_1_)    \n",
    "    - [Conclusion](#toc9_1_2_)    \n",
    "- [Experiment 2: Using a Multi-Query Retrieval Strategy](#toc10_)    \n",
    "    - [Results](#toc10_1_1_)    \n",
    "    - [Conclusion](#toc10_1_2_)    \n",
    "- [Experiment 3: Using a Step Back Strategy](#toc11_)    \n",
    "    - [Results](#toc11_1_1_)    \n",
    "    - [Conclusion](#toc11_1_2_)    \n",
    "- [Experiment 4: HyDE approach](#toc12_)    \n",
    "    - [Results](#toc12_1_1_)    \n",
    "    - [Conclusion](#toc12_1_2_)    \n",
    "- [Experiment 5: Contextual Compression](#toc13_)    \n",
    "    - [Results](#toc13_1_1_)    \n",
    "    - [Conclusion](#toc13_1_2_)    \n",
    "- [Overall Comparison](#toc14_)    \n",
    "  - [Comparative Evaluation Results](#toc14_1_)    \n",
    "  - [Aggregated Correctness vs. Precision](#toc14_2_)    \n",
    "  - [Aggregated Correctness vs. Reciprocal Rank](#toc14_3_)    \n",
    "  - [Confidence vs. Answer Correctness](#toc14_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfed57f",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[npr MC1: Cleantech Retrieval Augemented Generation](#toc0_)\n",
    "\n",
    "**Dominik Filliger, Nils Fahrni, Noah Leuenberger**\n",
    "\n",
    "> The topic of Mini-Challenge 1 is retrieval augmented generation (RAG) incorporating a combination of unsupervised learning, pre-training and in-context learning techniques.\n",
    "\n",
    "- [Description of the task](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/exercises/NPR-Mini-Challenge-1-Cleantech-RAG-1708982891.pdf)\n",
    "- [Introduction to RAG](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/exercises/Retrieval-Augmented-Generation-Intro-1709021241.pdf)\n",
    "\n",
    "\n",
    "# <a id='toc2_'></a>[Structure of our Solution](#toc0_)\n",
    "\n",
    "This notebook serves as the main entry point for our solution to the NPR Mini-Challenge 1. We will provide a detailed explanation of our approach and the code we used to solve the task. However, we have outsourced the code for the evaluation, Langchain LLM model creation and vectorstore interaction to script files which can be found in the `src` directory. We will reference these scripts in the respective sections and explain the code in place.\n",
    "\n",
    "Additionally, scripts for the development subset and subset evaluation set creation can be found in the `scripts` directory and will be referenced in their respective sections. These were primarily used to speed up the development process and are not necessary for the final solution as we will use the full dataset for the latest evaluation.\n",
    "\n",
    "The notebook starts by setting up a baseline pipeline. After that, we will conduct a series of experiments to explore the impact of different strategies on the performance of the pipeline. The experiments are designed to test the effectiveness of different retrieval and generation strategies in the RAG pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Setup](#toc0_)\n",
    "\n",
    "The setup section is used to import the necessary libraries and set the configuration for the notebook. We will also load the environment variables from the `.env` file to set the configuration for the notebook. The configuration includes the paths to the data files and the configuration for the Phoenix library.\n",
    "\n",
    "There is an option of three boolean variables which can be set to `True` or `False` to configure the notebook:\n",
    "- `DEV_MODE`: If set to `True`, the notebook will use the development subset of the data. If set to `False`, the notebook will use the full dataset.\n",
    "- `USE_GPU`: If set to `True`, the notebook will use the GPU for the computations. If set to `False`, the notebook will use the CPU.\n",
    "- `USE_CACHE`: If set to `True`, the notebook will use the cache for the computations. If set to `False`, the notebook will not use the cache.\n",
    "\n",
    "The caching is especially useful when running the notebook multiple times to avoid recomputing the same results. Especially the evaluation of the pipelines can be time-consuming and require a lot of tokens from the Azure API. Therefore, it is recommended to set `USE_CACHE` to `True` when running the notebook multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DEV_MODE = False\n",
    "USE_GPU = False\n",
    "USE_CACHE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Pathing](#toc0_)\n",
    "\n",
    "The pathing section is used to set the paths to the data files. The paths are set based on the `DEV_MODE` variable. If `DEV_MODE` is set to `True`, the paths to the development subset of the data are used. If `DEV_MODE` is set to `False`, the paths to the full dataset are used. The paths are then used to load the data files into pandas DataFrames. This is sole for convenience and to have a clear separation of the paths in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(dev_mode=DEV_MODE):\n",
    "    if dev_mode:\n",
    "        # Paths used in development mode\n",
    "        return {\n",
    "            'df_path': 'data/subset/cleantech_media_dataset_v2_2024-02-23_subset.csv',\n",
    "            'df_eval_path': 'data/subset/cleantech_media_dataset_v2_2024-02-23_subset_eval.csv',\n",
    "            'df_preprocessed_path': get_preprocessed_path(\n",
    "                'data/subset/cleantech_media_dataset_v2_2024-02-23_subset.csv'),\n",
    "            'df_eval_preprocessed_path': get_preprocessed_path(\n",
    "                'data/subset/cleantech_media_dataset_v2_2024-02-23_subset_eval.csv')\n",
    "        }\n",
    "    else:\n",
    "        # Paths used in production mode\n",
    "        return {\n",
    "            'df_path': 'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23.csv',\n",
    "            'df_eval_path': 'data/Cleantech Media Dataset/cleantech_rag_evaluation_data_2024-02-23.csv',\n",
    "            'df_preprocessed_path': get_preprocessed_path(\n",
    "                'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23.csv'),\n",
    "            'df_eval_preprocessed_path': get_preprocessed_path(\n",
    "                'data/Cleantech Media Dataset/cleantech_rag_evaluation_data_2024-02-23.csv')\n",
    "        }\n",
    "\n",
    "\n",
    "def get_preprocessed_path(path: str) -> str:\n",
    "    return path.replace(\".csv\", \"_preprocessed.csv\")\n",
    "\n",
    "\n",
    "paths = get_path()\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Observability & Monitoring](#toc0_)\n",
    "\n",
    "> Phoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n",
    "\n",
    "We will use Phoenix to visualize traces to quickly debug pipelines. The library offers way more feature which we will not use. Down below we add the Phoenix callbacks to Langchain, our main library for the solution, to visualize the traces.\n",
    "\n",
    "To get quick access to the Phoenix dashboard, open the following link in your browser: [http://localhost:6006](http://localhost:6006)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "import phoenix as px\n",
    "\n",
    "try:\n",
    "    px.close_app()\n",
    "    session = px.launch_app()\n",
    "except:\n",
    "    print(\n",
    "        \"Could not launch Phoenix app. Please make sure any existing Phoenix app process is closed. Restart the kernel if necessary.\")\n",
    "\n",
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Data Loading & Preprocessing](#toc0_)\n",
    "\n",
    "Next we will load the data and preprocess it. The data is loaded into a pandas DataFrame and then preprocessed. This step is based on the findings and explorations made in the EDA notebook (`notebooks/exploration.ipynb`). \n",
    "\n",
    "The dataset used in this Mini-Challenge is the Cleantech Media Dataset, which contains articles about clean technology and sustainability topics. The dataset consists of various columns, including the URL, domain, title, author, date, and content of the articles. The content column is of main interest as it contains the text data which the RAG piepline will be built upon. The dataset can be found [on Kaggle](https://www.kaggle.com/datasets/jannalipenkova/cleantech-media-dataset) together with an evaluation set which will be discussed later in the notebook.\n",
    "\n",
    "The preprocessing code itself can be found in the `src/preprocessing.py` file. The code is encapsulated in a class called `Preprocessor` which is used to preprocess the data which will be indexed and used for the retrieval augmented generation pipeline. There is also a separate class called `EvaluationPreprocessor` which is used to preprocess the evaluation set which will be discussed later in the notebook.\n",
    "\n",
    "The preprocessing includes the following steps:\n",
    "\n",
    "1. **Language Detection and Filtering:** Detect the language of each text chunk and filter out non-English chunks to maintain language consistency in the dataset.\n",
    "2. **HTML Cleaning:** Remove any HTML tags from the text content to ensure that only relevant textual data is processed.\n",
    "3. **Removing Special Characters:** Clean up the text by removing non-alphanumeric characters, keeping only letters, numbers, and necessary punctuation.\n",
    "4. **Duplicate Removal:** Identify and remove duplicate text chunks based on content to ensure uniqueness in the dataset.\n",
    "5. **Content Concatenation:** If the text content is split across multiple entries or rows, concatenate these into a single text chunk per dataset entry.\n",
    "6. **Adding Unique Identifiers:** Generate and append a unique identifier to each row based on the content's hash value, which helps in tracking and indexing the data for the evaluation process of the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(get_path()['df_path'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The preprocessed data is then saved to a new CSV file to avoid recomputing the preprocessing steps when running the notebook multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import Preprocessor\n",
    "\n",
    "if os.path.exists(paths['df_preprocessed_path']) and USE_CACHE:\n",
    "    df = pd.read_csv(paths['df_preprocessed_path'])\n",
    "else:\n",
    "    df = Preprocessor(df).preprocess()\n",
    "    df.to_csv(paths['df_preprocessed_path'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Indexing](#toc0_)\n",
    "\n",
    "The indexing step is used to embed the documents and store them in a vector store. The vector store is used to store the embeddings and provide an interface to interact with the embeddings.\n",
    "\n",
    "## <a id='toc5_1_'></a>[Chunking](#toc0_)\n",
    "\n",
    "Another important step in the indexing process is to split the documents into smaller chunks. This division is crucial for managing large documents, improving the granularity of search results, and enhancing the efficiency of the embedding process.\n",
    "\n",
    "**Purpose and Benefits**:\n",
    "\n",
    "- **Manageability**: Smaller chunks are easier to process and store, reducing the computational burden on the system.\n",
    "- **Precision**: By embedding smaller sections of text, the system can achieve more precise document retrieval, particularly for queries that target specific information within a large document.\n",
    "- **Performance**: Smaller text chunks lead to faster processing times during both embedding and retrieval phases.\n",
    "\n",
    "For the baseline pipeline, we will use the Recursive Character Text Splitter from the Langchain library. This splitter divides the text into smaller chunks based on the character count, ensuring that each chunk is of a manageable size for processing and embedding. The following parameters are used for the splitter:\n",
    "\n",
    "- **Chunk Size**: The maximum number of characters in each chunk.\n",
    "    - We use a chunk size of 1000 characters to ensure that the chunks are small enough for efficient processing but large enough to capture meaningful information. \n",
    "- **Chunk Overlap**: The number of characters that overlap between adjacent chunks.\n",
    "    - We set the overlap to 0 to avoid redundancy and ensure that each chunk contains unique information.   \n",
    "- **Length Function**: The function used to calculate the length of the text.\n",
    "- **Is Separator Regex**: A flag indicating whether the separator is a regular expression.\n",
    "\n",
    "The recursive character text splitter is a simple and effective method for dividing text into manageable chunks, making it suitable for our initial experiments with the RAG pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The helper function `create_documents` is used to create the documents from the DataFrame. The function takes the DataFrame and the text splitter as input and returns the documents. The function also prints the number of documents created and the percentage of documents created compared to the number of rows in the source DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_documents(df: pd.DataFrame, text_splitter, verbose=True):\n",
    "    metadata_cols = ['url', 'domain', 'title', 'author', 'date', 'id']\n",
    "    if not all(col in df.columns for col in metadata_cols + ['content']):\n",
    "        raise ValueError(\n",
    "            f\"DataFrame must contain all metadata columns and a 'content' column: {metadata_cols + ['content']}\")\n",
    "\n",
    "    metadata = df[metadata_cols].rename(columns={'id': 'origin_doc_id'}).to_dict('records')\n",
    "    for i, m in enumerate(metadata):\n",
    "        metadata[i] = {k: 'None' if v is None else v for k, v in m.items()}\n",
    "\n",
    "    docs = text_splitter.create_documents(df['content'], metadata)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"{text_splitter.__class__.__name__}: \"\n",
    "            f\"Number of documents created: {len(docs)}, \"\n",
    "            f\"Number of rows in source df: {len(df)}, \"\n",
    "            f\"Percentage of documents created: {len(docs) / len(df) * 100:.2f}%\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "documents = create_documents(df, recursive_text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed30c5f",
   "metadata": {},
   "source": [
    "Once split, each text chunk is processed through the BGE embeddings model to generate individual vectors. These vectors are then stored in the vector store. Handling multiple embeddings per document involves indexing each chunk separately while maintaining a reference to the original document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Embeddings](#toc0_)\n",
    "\n",
    "We will use the [BGE embeddings from Hugging Face](https://huggingface.co/BAAI/bge-small-en) to embed the documents. BGE embeddings are transformer-based models trained on a vast corpus of text data, making them highly effective for a variety of NLP tasks. The embeddings convert text data into dense vectors of fixed dimensionality, typically via processes including tokenization and normalization. The resulting vectors capture the semantic properties of the input texts.\n",
    "\n",
    "**Why specifically BGE embeddings?**\n",
    "\n",
    "- **High Semantic Fidelity**: BGE embeddings are designed to capture deep semantic meanings, enabling more effective document similarity checks and retrieval.\n",
    "- **Performance**: They are optimized for both accuracy and speed in large-scale applications.\n",
    "- **Versatility**: Suitable for diverse NLP tasks such as classification, clustering, and information retrieval.\n",
    "- **Accessibility through Langchain**: The Langchain library provides a simple interface to interact with the BGE embeddings, making it easier to integrate them into the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c546742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en\",\n",
    "    model_kwargs={\"device\": \"cpu\" if not USE_GPU else \"cuda:0\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Vector Store](#toc0_)\n",
    "\n",
    "We will use [ChromaDB](https://www.trychroma.com/) to store the embeddings. For easier interaction with the embeddings, we will use the `VectorStore` class which can be found in the `src/vector_store.py` file. The `VectorStore` class provides an interface that combines a specific collection of embeddings with the Langchain VectorStore API. The class allows for easy interaction with the embeddings, including adding documents and performing similarity searches.\n",
    "\n",
    "The embedding of the chunks takes time and resources, so we will use a persistent store to save the embeddings and avoid recomputing them when running the notebook multiple times. A database file with the precomputed embeddings can be downloaded and integrated according to the instructions in the README file.\n",
    "\n",
    "The collection name for the vector store is generated based on the embeddings model and the text splitter used. The collection name is used to identify the specific collection of embeddings in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from src.vector_store import VectorStore\n",
    "\n",
    "\n",
    "def get_vector_store_collection_name(embeddings, text_splitter):\n",
    "    return f\"cleantech-{embeddings.model_name}-{text_splitter.__class__.__name__}_{'dev' if DEV_MODE else 'prod'}\".lower().replace(\n",
    "        \" \", \"_\").replace(\"/\", \"_\").replace(\":\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "bge_vector_store = VectorStore(embedding_function=bge_embeddings,\n",
    "                               collection=get_vector_store_collection_name(bge_embeddings, recursive_text_splitter))\n",
    "\n",
    "get_vector_store_collection_name(bge_embeddings, recursive_text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the next step we will add the prepared documents from the previous step to the VectorStore. If the collection already exists in the VectorStore, this step is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bge_vector_store.add_documents(documents, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "After adding the documents to the vector store we can now perform similarity searches on the documents to verify that the interaction with the vector store works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bge_vector_store.similarity_search_w_scores(\"The company is also aiming to reduce gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4281d1e",
   "metadata": {},
   "source": [
    "The vector store returns a list of documents with their respective similarity scores. The documents are sorted by their similarity scores in descending order. The similarity scores indicate how similar the documents are to the query text. The higher the similarity score, the more similar the document is to the query text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Baseline Pipeline](#toc0_)\n",
    "\n",
    "The baseline pipeline is a first simple implementation of the RAG pipeline. As the name suggests, it serves as the starting point for our experiments and provides a reference for comparison with the subsequent pipelines. The baseline pipeline consists of the following components:\n",
    "\n",
    "- **Embeddings**: We use the BGE embeddings from Hugging Face to embed the documents.\n",
    "- **Retriever**: In the baseline pipeline, we use the default vector retriever from ChromaDB. It returns the top-4 most similar documents based on the cosine similarity.\n",
    "- **LLM Model**: We use the GPT-3.5 model provided on Azure as our LLM model. The model is responsible for generating the answer to the question based on the retrieved documents. The temperature parameter is set to the default of 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generation import get_llm_model, LLMModel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "base_retriever = bge_vector_store.get_retriever()\n",
    "azure_model = get_llm_model(LLMModel.GPT_3_AZURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "We will use the following prompt template to generate the chat prompt for the LLM model. The template includes the context and the question to provide the necessary information for the model to generate the answer and is fairly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "base_rag_prompt = \"\"\"\n",
    "Answer the question to your best knowledge when looking at the following context:\n",
    "{context}\n",
    "                \n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The `base_rag_chain` is the core of the baseline pipeline. It consists of the following steps:\n",
    "\n",
    "1. **Context Formatting**: The context documents are formatted into a single string to provide the necessary information to the LLM model.\n",
    "2. **Chat Prompt Generation**: The chat prompt is generated using the template defined above.\n",
    "3. **LLM Model Invocation**: The LLM model is invoked to generate the answer based on the context and the question.\n",
    "4. **Output Parsing**: The output from the LLM model is parsed to extract the answer from the response.\n",
    "5. **Answer Assignment**: The answer is assigned to the runnable parallel chain where it is combined with the context and the question. Context and question are also returned for further reference in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "base_rag_chain = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | ChatPromptTemplate.from_template(base_rag_prompt)\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "base_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": base_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "The `base_rag` pipeline/chain can now be used to generate answers for questions. We will test the pipeline with a sample question to verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rag.invoke(\"Is the company aiming to reduce gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb2f70",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Evaluation Setup](#toc0_)\n",
    "\n",
    "In order to compare the performance of different pipelines we need to evaluate them. The evaluation is done with the `ragas` library and classical non-LLM based metrics. \n",
    "\n",
    "`ragas` provides predefined metrics for the evaluation which are described in the [documentation](https://docs.ragas.io/en/stable/concepts/metrics/index.html). \n",
    "\n",
    "Further an overview of the metrics used in the evaluation is provided below:\n",
    "\n",
    "## <a id='toc7_1_'></a>[RAGAS Metrics](#toc0_)\n",
    "\n",
    "The `ragas` metrics are LLM-based evaluated metrics where the LLM is used to rate the generated answers and retrieved contexts against labeled ground truths. As LLM are known for their non-deterministic behavior, the metrics should be taken with a grain of salt. Nevertheless, they provide a relatively simple and straightforward way to evaluate the performance.\n",
    "\n",
    "1. [**Answer Correctness**](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html): This metric assesses the accuracy of the generated answer compared to the ground truth. It combines semantic and factual similarities between the answer and ground truth, providing a score ranging from 0 to 1. A higher score indicates that the generated answer closely aligns with the ground truth, reflecting a more correct response.\n",
    "\n",
    "2. [**Context Precision**](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html): This metric evaluates the relevance of the contexts retrieved by the RAG system in relation to the ground truth. It checks if the relevant items from the ground truth are ranked higher in the retrieved contexts. Scores range from 0 to 1, where higher values suggest better precision and relevance of the retrieved information.\n",
    "\n",
    "3. [**Answer Relevancy**](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html): Focused on the pertinence of the generated answer to the question, this metric scores based on how complete and free from redundant information the answer is. Higher scores indicate more relevant answers. It uses the mean cosine similarity between the original question and artificial questions generated based on the answer.\n",
    "\n",
    "4. [**Answer Semantic Similarity**](https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html): This metric evaluates the semantic resemblance between the generated answer and the ground truth. It uses a cross-encoder model to calculate semantic similarity, with scores between 0 and 1. Higher scores signify a better semantic alignment between the generated answer and the ground truth.\n",
    "\n",
    "5. [**Context Entity Recall**](https://docs.ragas.io/en/stable/concepts/metrics/context_entities_recall.html): This metric measures the recall of entities in the retrieved contexts against those in the ground truths. It is particularly useful in scenarios where factual accuracy and entity coverage are crucial, such as in historical QA or tourism help desks. The score is calculated by comparing the number of entities that overlap between the ground truths and contexts with the total number of entities in the ground truths.\n",
    "\n",
    "## <a id='toc7_2_'></a>[Non-LLM Based Metrics](#toc0_)\n",
    "\n",
    "In addition to the RAGAS metrics, we will also use the following metrics to evaluate the performance of the pipelines. They provide further specific insights into the retrieval performance.\n",
    "\n",
    "1. **Reciprocal Rank**: The reciprocal rank is a metric that evaluates the ranking of the correct answer in the list of retrieved answers. It is calculated as the reciprocal of the rank of the correct answer. A higher reciprocal rank indicates that the correct answer is ranked higher in the list of retrieved answers.\n",
    "2. **Hit@K**: The Hit@K metric evaluates whether the correct answer is present in the top K retrieved answers. It is calculated as a binary value, where 1 indicates that the correct answer is present in the top K retrieved answers, and 0 indicates otherwise. A higher Hit@K value indicates that the correct answer is more likely to be found in the top K retrieved answers\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_3_'></a>[Evaluation Data](#toc0_)\n",
    "\n",
    "The evaluation data is part of the datasets provided by the Mini-Challenge task [from Kaggle](https://www.kaggle.com/datasets/jannalipenkova/cleantech-media-dataset). The data consists of questions and a relevant chunk for each question.\n",
    "\n",
    "To evaluate the RAG system (and the following experiments) it is necessary to preprocess the evaluation dataset. For that purpose an `EvaluationPreprocessor` was built which ensures that the chunks of text extracted from a dataset during an evaluation are indeed the most relevant or closest matches to a known dataset. This is critical as we need to see if the RAG's outputs and evaluation's expected outputs (ground truth) match using the listed metrics. To calculate \"similarity\" or find matches the `EvaluationPreprocessor` calculates a similarity based on **fuzzy string matching**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(paths['df_eval_path'])\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import EvaluationPreprocessor\n",
    "\n",
    "if os.path.exists(paths['df_eval_preprocessed_path']) and USE_CACHE:\n",
    "    df_eval = pd.read_csv(paths['df_eval_preprocessed_path'])\n",
    "else:\n",
    "    df_eval = EvaluationPreprocessor(df, df_eval).preprocess()\n",
    "    df_eval.to_csv(paths['df_eval_preprocessed_path'], index=False)\n",
    "\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a5f31",
   "metadata": {},
   "source": [
    "After then `EvaluationPreprocessor` calculates the matches it adds the best score to each evaluation sample. The `best_match_id` represents the Id of the chunk which yielded the `best_match_score`, so this chunk seems to be the best matching chunk, according to the fuzzy matching, and should therefore be the chunk containing the information to answer the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_3_1_'></a>[Plausibility Checks](#toc0_)\n",
    "To see if the Baseline RAG Pipeline delivers *plausible* answers based on **handpicked questions** from the evaluation set the following lines will invoke the `base_rag` chain and ask these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDPICKED_QUESTIONS = [\n",
    "    \"Why does melting ice contribute to global warming?\",\n",
    "    \"In 2021, what were the top 3 states in the US in terms of total solar power generating capacity?\",\n",
    "    \"What is the EUâ€™s Green Deal Industrial Plan?\"\n",
    "]\n",
    "\n",
    "\n",
    "def ask_handpicked_questions(rag_chain, questions=HANDPICKED_QUESTIONS):\n",
    "    answers = {question: rag_chain.invoke(question) for question in questions}\n",
    "\n",
    "    for question, answer in answers.items():\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer['answer']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "ask_handpicked_questions(base_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "The Plausibility Check shows that the base RAG system indeed yields answers that are not out of the ordinary.\n",
    "**Note**: Through the LLM's non-deterministic nature it is possible that each run shows slightly different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from src.evaluation import Evaluator\n",
    "\n",
    "base_evaluator = Evaluator(name=\"Baseline\",\n",
    "                           cache_results=USE_CACHE,\n",
    "                           rag_chain=base_rag,\n",
    "                           llm_model=azure_model,\n",
    "                           embeddings=bge_embeddings)\n",
    "\n",
    "base_evaluator.evaluate(df_eval)\n",
    "base_evaluator.plot_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "The results of the RAGAS metrics show quite large differences for the Baseline RAG pipeline. The largest spread can be observed for the metrics `answer_correctness` and `answer_relevancy`. The Baseline pipeline seems to score quite low in **Context Entity Recall** with over half of the evaluation results being below 30%. The more stable metrics of **Answer Similarity** and **Context Precision** show quite large scores, ranging from just under 80% up to 99% with one outlier.\n",
    "\n",
    "The Baseline model seems to generally find the relevant chunks and seems to answer the questions semantically similar to the ground truths in the evaluation set. However, the low context entity recall indicates that the given answers don't exactly align well with the ground truths. We expect this to be a result of the LLM trying to come up with the correct answer without having the correct chunks of information at hand.\n",
    "\n",
    "Additionally the Reciprocal Rank (`rr`) and Hit@2 don't exactly show satisfyingly high scores. They both settle at around 50% (mean) so the retrieved chunks seem to not be consistently retrieved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_evaluator.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b9da5",
   "metadata": {},
   "source": [
    "When looking at the magnitude of the standard deviation of both the reciprocal rank and the hit@2 we can get a sense of their wide distributions. This observation indicates that the documents are in fact not consistent and vary greatly in evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Helper Functions](#toc0_)\n",
    "In order to make the notebook more readable and to avoid code duplication, we will define some helper functions that will be used throughout the notebook. These functions will help us to perform common tasks such as running experiments, evaluating pipelines, and visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560708be09f3c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_evaluator_results(evaluators):\n",
    "    \"\"\"\n",
    "    Combine evaluation results from multiple evaluators into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    evaluators (list of Evaluator): A list containing Evaluators.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A combined DataFrame containing all evaluation results with evaluator names.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If any evaluator's DataFrame is missing required columns or is None.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "\n",
    "    for evaluator in evaluators:\n",
    "        if evaluator.eval_results is None:\n",
    "            raise ValueError(f\"No evaluation results available for {evaluator.name}.\")\n",
    "\n",
    "        evaluator.eval_results['Evaluator'] = evaluator.name  # Add a column for the evaluator's name\n",
    "        combined_data.append(evaluator.eval_results)\n",
    "\n",
    "    final_df = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "    new_columns = {\n",
    "        'answer_correctness': 'Answer Correctness',\n",
    "        'answer_relevancy': 'Answer Relevancy',\n",
    "        'answer_similarity': 'Answer Similarity',\n",
    "        'context_precision': 'Context Precision',\n",
    "        'context_entity_recall': 'Context Entity Recall',\n",
    "        'rr': 'Reciprocal Rank',\n",
    "        'hit@2': 'Hit@2'\n",
    "    }\n",
    "    final_df = final_df.rename(columns=new_columns)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2f59a7cf527ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_evaluation_comparative_results(data, use_median=False, err_bars=True):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    columns_to_plot_bar = ['Answer Correctness', 'Answer Relevancy', 'Answer Similarity',\n",
    "                           'Context Precision', 'Context Entity Recall', 'Reciprocal Rank', 'Hit@2']\n",
    "\n",
    "    if use_median:\n",
    "        agg_method = 'median'\n",
    "        stats_df = (data[columns_to_plot_bar + ['Evaluator']]\n",
    "                    .groupby('Evaluator')\n",
    "                    .agg(['median', 'std'])\n",
    "                    .stack(level=0, future_stack=True)\n",
    "                    .reset_index())\n",
    "        value_col = 'median'\n",
    "    else:\n",
    "        agg_method = 'mean'\n",
    "        stats_df = (data[columns_to_plot_bar + ['Evaluator']]\n",
    "                    .groupby('Evaluator')\n",
    "                    .agg(['mean', 'std'])\n",
    "                    .stack(level=0, future_stack=True)\n",
    "                    .reset_index())\n",
    "        value_col = 'mean'\n",
    "\n",
    "    stats_df.columns = ['Evaluator', 'Metric', value_col, 'STD']\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    bars = sns.barplot(x='Metric', y=value_col, hue='Evaluator', data=stats_df, palette='Set2', capsize=.2)\n",
    "\n",
    "    if err_bars:\n",
    "        for bar, val, std in zip(bars.patches, stats_df[value_col], stats_df['STD']):\n",
    "            lower_bound = max(0, val - std)\n",
    "            upper_bound = min(1, val + std)\n",
    "            plt.errorbar(bar.get_x() + bar.get_width() / 2, val,\n",
    "                         yerr=[[val - lower_bound], [upper_bound - val]],\n",
    "                         fmt='none', color='black', capsize=5)\n",
    "\n",
    "    plt.title(f'{agg_method.capitalize()} Scores of Evaluation Metrics (N/metric={len(data)})')\n",
    "    plt.ylabel(f'{agg_method.capitalize()} Score')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Evaluator Name', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evaluation_comparative_results(data, use_median=False, err_bars=True):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(14, 16))  # Adjust size for better visualization\n",
    "\n",
    "    # Define the metrics to be plotted\n",
    "    columns_to_plot_bar = ['Answer Correctness', 'Answer Relevancy', 'Answer Similarity',\n",
    "                           'Context Precision', 'Context Entity Recall', 'Reciprocal Rank', 'Hit@2']\n",
    "    columns_to_plot_box = ['Answer Correctness', 'Answer Relevancy', 'Answer Similarity',\n",
    "                           'Context Precision', 'Context Entity Recall']  # Exclude 'Reciprocal Rank' and 'Hit@2'\n",
    "\n",
    "    # Aggregate data for bar plot using mean or median\n",
    "    if use_median:\n",
    "        agg_func = 'median'\n",
    "        value_col = 'median'\n",
    "    else:\n",
    "        agg_func = 'mean'\n",
    "        value_col = 'mean'\n",
    "\n",
    "    # Compute statistics for bar plot\n",
    "    stats_df = (data[columns_to_plot_bar + ['Evaluator']]\n",
    "                .groupby('Evaluator')\n",
    "                .agg([agg_func, 'std'])\n",
    "                .stack(level=0, future_stack=True)\n",
    "                .reset_index())\n",
    "    stats_df.columns = ['Evaluator', 'Metric', value_col, 'STD']\n",
    "\n",
    "    # Create subplots: 2 rows, 1 column\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(14, 18), gridspec_kw={'height_ratios': [1, 2]})\n",
    "\n",
    "    # Bar plot on the top subplot\n",
    "    bar_plot = sns.barplot(x='Metric', y=value_col, hue='Evaluator', data=stats_df, ax=ax[0], palette='Set2', capsize=.2)\n",
    "    ax[0].set_title(f'{agg_func.capitalize()} Scores of Evaluation Metrics')\n",
    "    ax[0].set_ylabel(f'{agg_func.capitalize()} Score')\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Add error bars if required\n",
    "    if err_bars:\n",
    "        for bar, val, std in zip(bar_plot.patches, stats_df[value_col], stats_df['STD']):\n",
    "            lower_bound = max(0, val - std)\n",
    "            upper_bound = min(1, val + std)\n",
    "            ax[0].errorbar(bar.get_x() + bar.get_width() / 2, val,\n",
    "                           yerr=[[val - lower_bound], [upper_bound - val]],\n",
    "                           fmt='none', color='black', capsize=5)\n",
    "\n",
    "    # Box plot on the bottom subplot for selected metrics\n",
    "    melted_df = pd.melt(data, id_vars=['Evaluator'], value_vars=columns_to_plot_box, var_name='Metric', value_name='Score')\n",
    "    box_plot = sns.boxplot(x='Metric', y='Score', hue='Evaluator', data=melted_df, ax=ax[1], palette='Set2')\n",
    "    ax[1].set_title('Distribution of Evaluation Metrics')\n",
    "    ax[1].set_ylabel('Score Distribution')\n",
    "    ax[1].set_xlabel('Metrics')\n",
    "    ax[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Adjust legend\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.05, 1), title='Evaluator')\n",
    "\n",
    "    # Remove legends from subplots to avoid duplication\n",
    "    ax[0].get_legend().remove()\n",
    "    ax[1].get_legend().remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use this function by passing a DataFrame to it with appropriate columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Experiment 1: Looking at the impact of context and its chunking strategy](#toc0_)\n",
    "\n",
    "The impact of chunking on the retrieval and generation performance is a crucial aspect of the RAG pipeline. The chunking strategy determines how the text is divided into smaller sections for processing and embedding. The choice of chunking strategy can significantly influence the granularity, relevance, and efficiency of the RAG system. Too big chunks might lead to irrelevant information being included in the retrieval and generation process, while too small chunks might result in fragmented information and loss of context.\n",
    "\n",
    "Given the baseline pipeline, which uses the recursive character chunking strategy with a chunk size of 1000 characters, we will now change this strategy to use smaller chunks of 200 characters each. The goal is to evaluate the impact of the chunking strategy on the retrieval and generation performance.\n",
    "\n",
    "When using smaller chunks we expect the following benefits:\n",
    "\n",
    "- **Increased granularity**: Smaller chunks allow for more precise retrieval and better matching of the query.\n",
    "- **Improved Relevance of Retrieved Content**: With smaller chunks, the system can more accurately match the query with specific parts of the documents that are most relevant. This could enhance the relevance of the content fed into the generation module of the RAG system, leading to more accurate and contextually appropriate responses.\n",
    "- **Reduced Noise**: Smaller chunks can help filter out irrelevant information and focus on the most critical parts of the text. This could improve the quality of the retrieved content and the generated answers.\n",
    "\n",
    "However, using smaller chunks might also introduce some challenges:\n",
    "- **Increased Processing Overhead**: Smaller chunks require more processing and embedding, which could lead to higher computational costs and longer processing times.\n",
    "- **Loss of Context**: Very small chunks might lose the context of the text, making it harder to understand the meaning and relevance of the content.\n",
    "- **Fragmentation**: Overly small chunks might fragment the text, making it difficult to piece together the information and generate coherent answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_text_splitter_200 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "recursive_200_documents = create_documents(df, recursive_text_splitter_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_200_vector_store = VectorStore(embedding_function=bge_embeddings,\n",
    "                                   collection=get_vector_store_collection_name(bge_embeddings,\n",
    "                                                                               recursive_text_splitter_200).replace(\n",
    "                                       \"recursivecharactertextsplitter\", \"recursive200\"))\n",
    "base_200_retriever = bge_200_vector_store.get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_200_vector_store.add_documents(recursive_200_documents, verbose=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_200_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": base_200_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)\n",
    "\n",
    "base_200_rag.invoke(\"Is the company aiming to reduce gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_1_1_'></a>[Results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_200_evaluator = Evaluator(name=\"Experiment 1: Baseline 200\",\n",
    "                               cache_results=USE_CACHE,\n",
    "                               rag_chain=base_200_rag,\n",
    "                               llm_model=azure_model,\n",
    "                               embeddings=bge_embeddings)\n",
    "\n",
    "base_200_evaluator.evaluate(df_eval)\n",
    "base_200_evaluator.plot_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d87eb42ee0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_combination = combine_evaluator_results([base_evaluator, base_200_evaluator])\n",
    "plot_evaluation_comparative_results(experiment_1_combination, use_median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "With the changes in chunking we can see an improvement for the **Answer Relevancy** metric. The new chunking strategy of chunking shorter sequences improve the way the Baseline RAG can process important information. Looking at the boxplot we can see that the spread of the **Answer Relevancy** metric is much smaller than in the Baseline approach. This indicates that the shorter chunks lead to more consistent results in the evaluation.\n",
    "\n",
    "An expected tradeoff therefore is that we get a little more spread in regard to the **Context Precision**. The increased spread might be an effect of the retriever falsely gathering chunks that contain similar wording but not the correct semantic meaning. And with that observation we can also expect the Reciprocal Rank and Hit@2 **not to improve** greatly since we add more options for the retriever to gather wrong while reducing noise within the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_200_evaluator.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_handpicked_questions(base_200_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5cc4c",
   "metadata": {},
   "source": [
    "The plausibility check shows the measured improvement: The first question of \"Why does melting ice contribute to global warming?\" now hits a relevant context and the LLM answers based on its information. The previous plausiblity check for the baseline system showed that the LLM did not have relevant context with which it could answer this question.\n",
    "\n",
    "The second and third question shows no difference; the LLM still answers the questions and has relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96f81a",
   "metadata": {},
   "source": [
    "### <a id='toc9_1_2_'></a>[Conclusion](#toc0_)\n",
    "Overall the hypotheses stated at the beginning of this experiment seem to align with the results we could gather during evaluation. The slightly \"worse\" Context Precision is a tradeoff that didn't show a drastically negative impact on the system, therefore we can say that the shorter chunks do have a positive impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[Experiment 2: Using a Multi-Query Retrieval Strategy](#toc0_)\n",
    "\n",
    "At the heart of the RAG is the retriever, which is responsible for finding the most relevant documents for a given question. The baseline RAG uses the vector retriever to find the most relevant document, using **cosine-similarity**. \n",
    "\n",
    "We will now experiment with a **multi-query retrieval strategy**. The idea is to use multiple queries to retrieve a multitude of documents and take a unique union of the results. This way we can increase the diversity of the documents and potentially improve the quality of the generated answer. \n",
    "\n",
    "![image.png](./notebooks/images/multiqueryretrieval.png)\n",
    "\n",
    "For this we will use the `MultiQueryRetriever` from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "mqr_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever, llm=azure_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "To implement this new strategy we need to update the LLM with the new instruction of generating `5` additional different versions of the users given question. The instruction also states the goal of why this approach should be useful for chunk retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "        prompt_perspectives\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(union_docs: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in union_docs for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "mqr_retrieval_chain = (\n",
    "        generate_queries\n",
    "        | mqr_retriever.map()\n",
    "        | get_unique_union\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mqr_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": mqr_retrieval_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqr_rag.invoke(\"Is the company aiming to reduce gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc10_1_1_'></a>[Results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mqr_evaluator = Evaluator(name=\"Experiment 2: Multi-Query Retrieval\",\n",
    "                          cache_results=USE_CACHE,\n",
    "                          rag_chain=mqr_rag,\n",
    "                          llm_model=azure_model,\n",
    "                          embeddings=bge_embeddings)\n",
    "\n",
    "mqr_evaluator.evaluate(df_eval)\n",
    "mqr_evaluator.plot_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fcc819e1c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_2_combination = combine_evaluator_results([base_evaluator, base_200_evaluator, mqr_evaluator])\n",
    "plot_evaluation_comparative_results(experiment_2_combination, use_median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "This experiment shows a new improvement in the **Answer Correctness** metric. This metric's distribution had a vast spread in the previous experiments. The improvement of the Multi-Query Retrieval squished this metric's distribution towards improved scores. In contrast to this improvement the **Context Precision** gained a large amount of spread which makes sense given that we retrieve more chunks with more questions.\n",
    "\n",
    "The ranking metrics `rr` and `hit@2` also yield a lower score which is an expected observation with the large amount of documents that we now retrieve. It could also mean that we are generating too many questions but don't have enough documents that can answer these questions - We therefore reintroduce more noise in the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqr_evaluator.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_handpicked_questions(mqr_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702ab6d",
   "metadata": {},
   "source": [
    "The plausibility check shows that the LLM gained more context and detailed information which seemed to result in the Azure model generating more insights, especially for question 1 and 3. Question 2 answered the same as in the first experiment with the Baseline system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa18ea5",
   "metadata": {},
   "source": [
    "### <a id='toc10_1_2_'></a>[Conclusion](#toc0_)\n",
    "We can see clear improvement in the realm of answer correctness though we also seem to lose context precision. In a real world application it would have definitely also made sense to look at how this Multi-Query Retrieval behaves with shorter chunk sizes (combining Experiment 1 and 2). The five additional questions resulted in much more retrieved documents with potentially adds too much noise for the LLM to extract important information from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[Experiment 3: Using a Step Back Strategy](#toc0_)\n",
    "The Step Back Strategy was first proposed by the Google DeepMind Paper \"[TAKE A STEP BACK: EVOKING REASONING VIA ABSTRACTION IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2310.06117)\". The paper presents a novel way to prompt LLMs with a new 2-step approach:\n",
    "1. **Abstraction Step**: The first step is to prompt the LLM to \"step back\" and extract the key principles, concepts or patterns from the given details, rather than focusing on the specifics. For example, for a physics question, the step-back question could be \"What are the relevant physics principles at play here?\".\n",
    "2. **Utilizing Abstracted Knowledge**: The second step is to have the LLM use the abstracted knowledge gained from the step-back question to guide its reasoning and arrive at the final solution. This allows the LLM to focus on the core aspects of the task and avoid getting distracted by irrelevant details.\n",
    "\n",
    "![image.png](./notebooks/images/stepback.png)\n",
    "\n",
    "We expect the following benefits in the Step-Back Approach:\n",
    "- **More concrete reasoning**: By \"taking a step back\" the LLM will be able to understand the core concepts of the underlying question. The LLM should therefore have \"the bigger picture\".\n",
    "- **Contextual relevancy**: Stepping back also helps to gain a better overview on a topic which should help the LLM get a better sense of what's relevant and what's not.\n",
    "\n",
    "In contrast to the benefits the following disadvantage is expected:\n",
    "- **Loss of detail**: If a question is already very contextually precise taking a step back and adding a less detailed view might not be beneficial but rather introduce unwanted noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindelâ€™s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindelâ€™s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "generate_queries_step_back = (prompt | azure_model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "response_prompt_template = \"\"\"\n",
    "You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "\n",
    "def get_unique_documents(documents):\n",
    "    \"\"\"\n",
    "    Combines and filters documents from 'normal_context' and 'step_back_context' to ensure uniqueness.\n",
    "\n",
    "    This function merges a list of documents, eliminating any duplicates based on the 'page_content' attribute.\n",
    "    It assumes that each document in the lists is an object with a 'page_content' attribute that contains the unique content of the document.\n",
    "\n",
    "    Parameters:\n",
    "    - documents (list): A list of Langchain document objects.\n",
    "      Each key corresponds to a list of document objects with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of unique document objects based on their 'page_content'.\n",
    "    \"\"\"\n",
    "    seen_content = set()\n",
    "    combined_docs = []\n",
    "    for doc in documents:\n",
    "        if doc.page_content not in seen_content:\n",
    "            combined_docs.append(doc)\n",
    "            seen_content.add(doc.page_content)\n",
    "    return combined_docs\n",
    "\n",
    "\n",
    "step_back_retrieval_chain = (\n",
    "    {\n",
    "        \"normal_context\": RunnableLambda(lambda x: x) | base_retriever,\n",
    "        \"step_back_context\": generate_queries_step_back | base_retriever,\n",
    "    }\n",
    ")\n",
    "\n",
    "step_back_answer_chain = (\n",
    "        response_prompt\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "step_back_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": step_back_retrieval_chain | RunnableLambda(\n",
    "            lambda x: get_unique_documents(x[\"normal_context\"] + x[\"step_back_context\"])),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=step_back_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_rag.invoke(\"What is the company's goal in reducing gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc11_1_1_'></a>[Results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_evaluator = Evaluator(name=\"Experiment 3: Step Back\",\n",
    "                                cache_results=USE_CACHE,\n",
    "                                rag_chain=step_back_rag,\n",
    "                                llm_model=azure_model,\n",
    "                                embeddings=bge_embeddings)\n",
    "\n",
    "step_back_evaluator.evaluate(df_eval)\n",
    "step_back_evaluator.plot_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a98ea49fc8fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3_combination = combine_evaluator_results([base_evaluator, base_200_evaluator, mqr_evaluator, step_back_evaluator])\n",
    "plot_evaluation_comparative_results(experiment_3_combination, use_median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "An interesting observation is that the **Answer Correctness** seems to have stayed the same in its distribution shape but shifted up towards a higher score. The decrease in the median score of **Answer Correctness** might indicate that while the answers are correct, they might not be as closely aligned to the specifics of the query as before. This shift could be due to the abstraction step, which focuses on general principles rather than the finer details of the question, potentially leading to answers that are correct in a broader sense but less tailored to the specific query context.\n",
    "\n",
    "Also interesting to point out is how **Context Precision** stayed about the same, where Multi-Query Retrieval showed a decrease in this metric. This could indicate that the Step Back Strategy is more effective in filtering out irrelevant information and focusing on the core concepts of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_evaluator.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_handpicked_questions(step_back_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52620d",
   "metadata": {},
   "source": [
    "When taking a closer look at the plausibility check the handpicked questions show a new observation for all 3 questions: The answers are much longer and go in thorough detail on the concepts in the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55ca71",
   "metadata": {},
   "source": [
    "### <a id='toc11_1_2_'></a>[Conclusion](#toc0_)\n",
    "To conclude this Experiment, the Step Back Strategy appears to enhance the precision of the retrieval process by emphasizing core concepts and principles over specific details. However, this focus leads to slightly less tailored detailed answers, as seen in the reduced relevancy and similarity scores. These observations underscore the trade-off between abstract reasoning and the granularity of response specificity, which might need further tuning to optimize both the relevance and correctness of the answers.\n",
    "\n",
    "The experiment showed clear effects on the plausibility check as the LLM really elaborated on the superficial concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[Experiment 4: HyDE approach](#toc0_)\n",
    "\n",
    "The HyDE approach introduces a method called Hypothetical Document Embeddings (HyDE) for enhancing zero-shot dense retrieval. Originally proposed by Gao et al. (2022) in the paper [\"Precise Zero-Shot Dense Retrieval without Relevance Label\"](https://arxiv.org/abs/2212.10496). \n",
    "\n",
    "In the HyDE approach, an instruction-following language model, such as our Azure Model using the right prompting, is used to generate a hypothetical document in response to a query. This document is designed to capture the essential relevance patterns from the input question potentially containing inaccuracies or hypothetical details. Following this, the document is used to retrieve relevant documents from the vector store, which are then passed to the LLM model for answer generation.\n",
    "\n",
    "This two-step process allows the HyDE model to ground the generated hypothetical content in actual data, effectively filtering out incorrect or irrelevant details through the embedding's bottleneck.\n",
    "\n",
    "![image.png](./notebooks/images/hyde.png)\n",
    "\n",
    "The graphic might falsely suggest that the hypothetical document will be embedded but it is merely used to query during the retrieval in inference-time. But the illustration shows the hypothesis that an LLM-generated hypothetical document may already be closer to the relevant chunks in the embedding space as it is much richer with relevant information that the original input question might not capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system = \"\"\"You are an expert about the Clean Technology Sector.\n",
    "            Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question.\"\"\"\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_hypothetical_doc = (\n",
    "        hyde_prompt\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec59a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_retrieval_chain = (gen_hypothetical_doc\n",
    "                        | base_retriever\n",
    "                        )\n",
    "\n",
    "hyde_rag = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hyde_retrieval_chain\n",
    "    }\n",
    ").assign(answer=base_rag_chain)\n",
    "\n",
    "hyde_rag.invoke(\"What is the company's goal in reducing gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc12_1_1_'></a>[Results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_evaluator = Evaluator(name=\"Experiment 4: HyDE\",\n",
    "                           cache_results=USE_CACHE,\n",
    "                           rag_chain=hyde_rag,\n",
    "                           llm_model=azure_model,\n",
    "                           embeddings=bge_embeddings)\n",
    "\n",
    "hyde_evaluator.evaluate(df_eval)\n",
    "hyde_evaluator.plot_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27de90ea839b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_4_combination = combine_evaluator_results([base_evaluator, base_200_evaluator, mqr_evaluator, step_back_evaluator, hyde_evaluator])\n",
    "plot_evaluation_comparative_results(experiment_4_combination, use_median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "The results of this 4th experiment show very similar metrics to the baseline RAG which is interesting as the HyDE approach actually should show a more meaningful ability in the retrieval.\n",
    "\n",
    "If we just focus on comparing the HyDE approach with the Baseline RAG we can see a positive shift in the **Answer Correctness** metric which is expected as we improve the retrieval strategy by giving the retrieval query a more detailed chunk to query with.\n",
    "\n",
    "Compared with the other approaches (Experiment 1 to 3) this approach seems to not improve the RAG system in a more meaningful way on multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_evaluator.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_handpicked_questions(hyde_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c1830",
   "metadata": {},
   "source": [
    "The plausibility check in contrast to the Baseline RAG that the first question's context can be retrieved and therefore be answered. The second question seems to stay the same while the third question shows a concise but exact answer which seems plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef95068",
   "metadata": {},
   "source": [
    "### <a id='toc12_1_2_'></a>[Conclusion](#toc0_)\n",
    "The HyDE experiment showed a slight improvement in **Answer Correctness** compared to the metrics in the Baseline RAG. Compared to the other approaches though, this approach rather falls short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc13_'></a>[Experiment 5: Contextual Compression](#toc0_)\n",
    "Contextual Compression addresses the challenge in information retrieval of optimizing how relevant information is extracted from large datasets or documents. The relevant information for some specific query may well be buried in a larger chunk with surrounding text that isn't really relevant to the input question. We hypothesize that this fact will lead to a poorer performance of of the RAG system as the LLM will have much more information on its hand but will have a hard time attending the relevant part of the retrieved document because it contains a lot of noise. \n",
    "\n",
    "Contextual Compression however attempts to solve this issue by first compressing the retrieved documents to their \"crucial parts\" that actually contain the context-relevant information before using them to generate an answer. The `ContextualCompressionRetriever` will not only compress the contents of each document but also remove or \"compress\" entire documents that may show to not be valuable in their context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "chain_extractor_compressor = LLMChainExtractor.from_llm(azure_model)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=chain_extractor_compressor, base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "base_rag_chain_compression = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | ChatPromptTemplate.from_template(base_rag_prompt)\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "base_rag_compression = RunnableParallel(\n",
    "    {\n",
    "        \"context\": compression_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain_compression)\n",
    "\n",
    "base_rag_compression.invoke(\"Is the company aiming to reduce gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc13_1_1_'></a>[Results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_compression_evaluator = Evaluator(name=\"Experiment 5: Contextual Compression\",\n",
    "                                       cache_results=USE_CACHE,\n",
    "                                       rag_chain=base_rag_compression,\n",
    "                                       llm_model=azure_model,\n",
    "                                       embeddings=bge_embeddings)\n",
    "\n",
    "base_compression_evaluator.evaluate(df_eval)\n",
    "base_compression_evaluator.plot_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76209cee8fd3da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_5_combination = combine_evaluator_results([base_evaluator, base_200_evaluator, mqr_evaluator, step_back_evaluator, hyde_evaluator, base_compression_evaluator])\n",
    "plot_evaluation_comparative_results(experiment_5_combination, use_median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "The most drastic observation that instantly arises is the large interquartile range of the **Answer Relevancy**. It suggests that the contextual compression might have been too harsh in the compression of information. Compared to the Baseline RAG system it doesn't show a change in any other metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_compression_evaluator.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_handpicked_questions(base_rag_compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd29c7",
   "metadata": {},
   "source": [
    "The first question gets answered very generally, perhaps without the inclusion of chunked information. The second question however seems to not have any context which was used by the LLM to answer the question. It rather seems made up by the LLMs knowledge. The third question is however again more regular and gets answered with much more detail and is therefore very plausible to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87096a62",
   "metadata": {},
   "source": [
    "### <a id='toc13_1_2_'></a>[Conclusion](#toc0_)\n",
    "The extreme increase in the **Answer Relevancy**'s spread can possibly already be seen in the plausibility check. The check showed that for some of the questions this approach still found relevant chunks and answered rich in detail while in other cases it struggled with detail and even with finding relevant context. It can be assumed that the compression was just too harsh and got rid of too much important granular detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc14_'></a>[Overall Comparison](#toc0_)\n",
    "\n",
    "In this section, we will compare the performance of the different pipelines based on the evaluation metrics. The comparison will provide insights into the effectiveness of each pipeline in generating accurate and relevant answers to the questions. The evaluation metrics include both LLM-based metrics and non-LLM-based metrics to provide a comprehensive assessment of the pipelines' performance\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ce6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = [base_evaluator,\n",
    "              base_200_evaluator,\n",
    "              mqr_evaluator,\n",
    "              step_back_evaluator,\n",
    "              hyde_evaluator,\n",
    "              base_compression_evaluator]\n",
    "\n",
    "combined_results = combine_evaluator_results(evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc14_1_'></a>[Comparative Evaluation Results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_evaluation_comparative_results(combined_results, use_median=False, err_bars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "- **HyDE** demonstrates relative strong performance across correctness, relevancy, and precision metrics, highlighting its capability to leverage synthetic documents effectively for high-quality information retrieval.\n",
    "\n",
    "- **Baseline 200 (Shorter Chunking)** shows consistently high performance, especially in ranking metrics and answer similarity,  robust retrieval and matching capabilities.\n",
    "\n",
    "- The **Step Back** strategy shines in its stronger capabilities in the Answer Correctness and Context Precision which can be achieved with the larger overview on subjects.\n",
    "\n",
    "- The **Multi-Query Retrieval** achieves the best overall scores in Answer Relevancy and Answer Similarity but struggles within the ranking specific metrics due to its great variance in different retrieved document chunks.\n",
    "\n",
    "- The default **Baseline RAG** shows to be a good middle ground amidst all the explored alterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation_comparative_results(combined_results, use_median=True, err_bars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "The median in contrast to the mean is stable among the experimented approaches in regards to Answer Relevancy, Answer Similarity and Context Entity Recall. \n",
    "\n",
    "Within the Answer Correctness the **HyDE** approach seems to differentiate itself the most from the other explored methods with its negative skew in the distribution.\n",
    "\n",
    "The **Multi-Query Retrieval** stands out particularly in the Context Precision, Reciprocal Rank and Hit@2 where it scores significantly lower than the other explored methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_boxplots(data):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    metrics = [\n",
    "        'Answer Correctness', 'Answer Relevancy', 'Answer Similarity',\n",
    "        'Context Precision', 'Context Entity Recall'\n",
    "    ]\n",
    "\n",
    "    data_filtered = data[metrics + ['Evaluator']]\n",
    "    data_long = pd.melt(data_filtered, id_vars=['Evaluator'], value_vars=metrics,\n",
    "                        var_name='Metric', value_name='Value')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(\n",
    "        x='Metric',\n",
    "        y='Value',\n",
    "        hue='Evaluator',\n",
    "        data=data_long,\n",
    "        palette='Set2'\n",
    "    )\n",
    "\n",
    "    plt.title('Distribution of Evaluation Metrics Across Evaluators')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Evaluator', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_metric_boxplots(combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ba4ad",
   "metadata": {},
   "source": [
    "When looking at the distributions of all methods pretty much all approaches show a wide spread in the Answer Correctness, Context Precision and Context Entity Recall. Within this observation the **Step Back** approach seems to be the most stable with the least spread in all boxplots.\n",
    "\n",
    "The **Contextual Compression** and **Baseline RAG** approach seem to struggle the most with consistency as their metrics have a large spread in almost all metrics but the Answer Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc14_2_'></a>[Aggregated Correctness vs. Precision](#toc0_)\n",
    "\n",
    "By plotting these metrics against each other, stakeholders can quickly discern whether there is a correlation between the precision of the context provided to the generator and the accuracy of the answers generated. For example, a positive correlation would suggest that improving context precision could lead to more accurate answers.\n",
    "\n",
    "Each point on the scatter plot represents an evaluator, with the x-axis showing the average correctness of the answers generated and the y-axis showing the average precision of the context provided to the generator. The color of the points corresponds to the evaluator, allowing for easy identification of each evaluator's performance. The point values is the mean of the respective metric across all questions in the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_correctness_vs_precision(data):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    aggregated_data = data.groupby('Evaluator').agg({\n",
    "        'Answer Correctness': 'mean',\n",
    "        'Context Precision': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x='Answer Correctness',\n",
    "        y='Context Precision',\n",
    "        hue='Evaluator',\n",
    "        data=aggregated_data,\n",
    "        palette='Set2',\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "    plt.title('Mean Correctness vs. Precision across Evaluators')\n",
    "    plt.xlabel('Average Answer Correctness (Accuracy)')\n",
    "    plt.ylabel('Average Context Precision (Relevance)')\n",
    "    plt.legend(title='Evaluator', bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_aggregated_correctness_vs_precision(combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3bdfc",
   "metadata": {},
   "source": [
    "There doesn't seem to be any particular clear correlation. The **Step Back** shows to be performing the best when it comes to the combination of the Accuracy and Relevance.\n",
    "\n",
    "When looking at all the other approaches one could imagine the both metrics to be negatively correlated as the ones with high Relevance show lower Accuracy and vice versa. Though it's hard to say if this holds up - The **Step Back** approach for example doesn't follow this pattern so it's fair to say that it depends very much on what an approach actually implements and improves on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc14_3_'></a>[Aggregated Correctness vs. Reciprocal Rank](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_correctness_vs_rr(data):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    aggregated_data = data.groupby('Evaluator').agg({\n",
    "        'Answer Correctness': 'mean',\n",
    "        'Reciprocal Rank': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x='Answer Correctness',\n",
    "        y='Reciprocal Rank',\n",
    "        hue='Evaluator',\n",
    "        data=aggregated_data,\n",
    "        palette='Set2',\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "    plt.title('Mean Correctness vs. Reciprocal Rank across Evaluators')\n",
    "    plt.xlabel('Mean Answer Correctness')\n",
    "    plt.ylabel('Mean Reciprocal Rank')\n",
    "    plt.legend(title='Evaluator', bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_aggregated_correctness_vs_rr(combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd79a9f",
   "metadata": {},
   "source": [
    "The same observation can be done here, the **Step Back** approach reaches the highest MRR and Mean Answer Correctness. The other experiments, on the other hand, fall out lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc14_4_'></a>[Confidence vs. Answer Correctness](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_vs_correctness(data):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    metrics = [\n",
    "        'Answer Correctness', 'Answer Relevancy', 'Answer Similarity',\n",
    "        'Context Precision', 'Context Entity Recall', 'Reciprocal Rank', 'Hit@2'\n",
    "    ]\n",
    "\n",
    "    stats = data.groupby('Evaluator')[metrics].agg(['mean', 'std'])\n",
    "\n",
    "    stats['Avg STD'] = stats[[(m, 'std') for m in metrics]].mean(axis=1)\n",
    "    max_std = stats['Avg STD'].max()\n",
    "    stats['System Confidence'] = max_std + 1 - stats['Avg STD']\n",
    "\n",
    "    stats.columns = [' '.join(col).strip() for col in stats.columns.values]\n",
    "    stats.reset_index(inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x='System Confidence',\n",
    "        y='Answer Correctness mean',\n",
    "        hue='Evaluator',\n",
    "        data=stats,\n",
    "        s=100,\n",
    "        palette='Set2',\n",
    "        legend='full'\n",
    "    )\n",
    "\n",
    "    plt.title('System Confidence vs. Answer Correctness')\n",
    "    plt.xlabel('System Confidence (Inferred from Avg. STD of Metrics)')\n",
    "    plt.ylabel('Answer Correctness (Mean)')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title='Evaluator', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_confidence_vs_correctness(combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "The last plot shows the mean of the Answer Correctness and the average standard deviation of all metrics (or System Confidence). Here we can see that no linear relationship between the two is visible.\n",
    "\n",
    "The **Multi-Query Retrieval** approach might shine through as the best option when just looking at the System Confidence in combination with Answer Correctness but it is only around 0.03 units better compared with the **Step Back** approach which excelled in the other observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
