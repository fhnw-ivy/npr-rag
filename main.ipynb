{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393b298e",
   "metadata": {},
   "source": [
    "# npr MC1: Cleantech Retrieval Augemented Generation\n",
    "\n",
    "**Dominik Filliger, Nils Fahrni, Noah Leuenberger**\n",
    "\n",
    "> The topic of Mini-Challenge 1 is retrieval augmented generation (RAG) incorporating a combination of unsupervised learning, pre-training and in-context learning techniques.\n",
    "\n",
    "- [Description of the task](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/exercises/NPR-Mini-Challenge-1-Cleantech-RAG-1708982891.pdf)\n",
    "- [Introduction to RAG](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/exercises/Retrieval-Augmented-Generation-Intro-1709021241.pdf)\n",
    "\n",
    "\n",
    "# Structure of our Solution\n",
    "\n",
    "This notebook serves as the main entry point for our solution to the NPR Mini-Challenge 1. We will provide a detailed explanation of our approach and the code we used to solve the task. However, we have outsourced the code for the evaluation, Langchain LLM model creation and vectorstore interaction to script files which can be found in the `src` directory. We will reference these scripts in the respective sections and explain the code in place.\n",
    "\n",
    "Additionally, scripts for the development subset and subset evaluation set creation can be found in the `scripts` directory and will be referenced in their respective sections. These were primarily used to speed up the development process and are not necessary for the final solution as we will use the full dataset for the latest evaluation.\n",
    "\n",
    "The notebook starts by setting up a baseline pipeline. After that, we will conduct a series of experiments to explore the impact of different strategies on the performance of the pipeline. The experiments are designed to test the effectiveness of different retrieval and generation strategies in the RAG pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c7169756adcbe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "The setup section is used to import the necessary libraries and set the configuration for the notebook. We will also load the environment variables from the `.env` file to set the configuration for the notebook. The configuration includes the paths to the data files and the configuration for the Phoenix library.\n",
    "\n",
    "There is an option of three boolean variables which can be set to `True` or `False` to configure the notebook:\n",
    "- `DEV_MODE`: If set to `True`, the notebook will use the development subset of the data. If set to `False`, the notebook will use the full dataset.\n",
    "- `USE_GPU`: If set to `True`, the notebook will use the GPU for the computations. If set to `False`, the notebook will use the CPU.\n",
    "- `USE_CACHE`: If set to `True`, the notebook will use the cache for the computations. If set to `False`, the notebook will not use the cache.\n",
    "\n",
    "The caching is especially useful when running the notebook multiple times to avoid recomputing the same results. Especially the evaluation of the pipelines can be time-consuming and require a lot of tokens from the Azure API. Therefore, it is recommended to set `USE_CACHE` to `True` when running the notebook multiple times."
   ]
  },
  {
   "cell_type": "code",
   "id": "848fa0cdf9b39b57",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DEV_MODE = False\n",
    "USE_GPU = False\n",
    "USE_CACHE = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pathing\n",
    "\n",
    "The pathing section is used to set the paths to the data files. The paths are set based on the `DEV_MODE` variable. If `DEV_MODE` is set to `True`, the paths to the development subset of the data are used. If `DEV_MODE` is set to `False`, the paths to the full dataset are used. The paths are then used to load the data files into pandas DataFrames. This is sole for convenience and to have a clear separation of the paths in one place."
   ],
   "id": "9a252147624c24dc"
  },
  {
   "cell_type": "code",
   "id": "311e37f25647c953",
   "metadata": {},
   "source": [
    "def get_path(dev_mode=DEV_MODE):\n",
    "    if dev_mode:\n",
    "        # Paths used in development mode\n",
    "        return {\n",
    "            'df_path': 'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23_subset.csv',\n",
    "            'df_eval_path': 'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23_subset_eval.csv',\n",
    "            'df_preprocessed_path': get_preprocessed_path(\n",
    "                'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23_subset.csv'),\n",
    "            'df_eval_preprocessed_path': get_preprocessed_path(\n",
    "                'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23_subset_eval.csv')\n",
    "        }\n",
    "    else:\n",
    "        # Paths used in production mode\n",
    "        return {\n",
    "            'df_path': 'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23.csv',\n",
    "            'df_eval_path': 'data/Cleantech Media Dataset/cleantech_rag_evaluation_data_2024-02-23.csv',\n",
    "            'df_preprocessed_path': get_preprocessed_path(\n",
    "                'data/Cleantech Media Dataset/cleantech_media_dataset_v2_2024-02-23.csv'),\n",
    "            'df_eval_preprocessed_path': get_preprocessed_path(\n",
    "                'data/Cleantech Media Dataset/cleantech_rag_evaluation_data_2024-02-23.csv')\n",
    "        }\n",
    "\n",
    "\n",
    "def get_preprocessed_path(path: str) -> str:\n",
    "    return path.replace(\".csv\", \"_preprocessed.csv\")\n",
    "\n",
    "\n",
    "paths = get_path()\n",
    "print(paths)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47adf37081d73c21",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Observability & Monitoring\n",
    "\n",
    "> Phoenix is an open-source observability library designed for experimentation, evaluation, and troubleshooting. It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.\n",
    "\n",
    "We will use Phoenix to visualize traces to quickly debug pipelines. The library offers way more feature which we will not use. Down below we add the Phoenix callbacks to Langchain, our main library for the solution, to visualize the traces.\n",
    "\n",
    "To get quick access to the Phoenix dashboard, open the following link in your browser: [http://localhost:6006](http://localhost:6006)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d0092013e36ca8c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "import phoenix as px\n",
    "\n",
    "try:\n",
    "    px.close_app()\n",
    "    session = px.launch_app()\n",
    "except:\n",
    "    print(\n",
    "        \"Could not launch Phoenix app. Please make sure any existing Phoenix app process is closed. Restart the kernel if necessary.\")\n",
    "\n",
    "LangChainInstrumentor().instrument()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f9f2033ded77748",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data Loading & Preprocessing\n",
    "\n",
    "Next we will load the data and preprocess it. The data is loaded into a pandas DataFrame and then preprocessed. This step is based on the findings and explorations made in the EDA notebook (`notebooks/exploration.ipynb`). \n",
    "\n",
    "The preprocessing code itself can be found in the `src/preprocessing.py` file. The code is encapsulated in a class called `Preprocessor` which is used to preprocess the data which will be indexed and used for the retrieval augmented generation pipeline. There is also a separate class called `EvaluationPreprocessor` which is used to preprocess the evaluation set which will be discussed later in the notebook.\n",
    "\n",
    "The preprocessing includes the following steps:\n",
    "\n",
    "// TODO: Add preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "id": "d028a391597bb1de",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(get_path()['df_path'])\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The preprocessed data is then saved to a new CSV file to avoid recomputing the preprocessing steps when running the notebook multiple times.",
   "id": "f83eeba096da3787"
  },
  {
   "cell_type": "code",
   "id": "1d6d6b974dc56bb8",
   "metadata": {},
   "source": [
    "from src.preprocessing import Preprocessor\n",
    "\n",
    "if os.path.exists(paths['df_preprocessed_path']) and USE_CACHE:\n",
    "    df = pd.read_csv(paths['df_preprocessed_path'])\n",
    "else:\n",
    "    df = Preprocessor(df).preprocess()\n",
    "    df.to_csv(paths['df_preprocessed_path'], index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29f172af1ea76470",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Indexing\n",
    "\n",
    "The indexing step is used to embed the documents and store them in a vector store. The vector store is used to store the embeddings and provide an interface to interact with the embeddings.\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "We will use the [BGE embeddings from Hugging Face](https://huggingface.co/BAAI/bge-small-en) to embed the documents. BGE embeddings are transformer-based models trained on a vast corpus of text data, making them highly effective for a variety of NLP tasks. The embeddings convert text data into dense vectors of fixed dimensionality, typically via processes including tokenization and normalization. The resulting vectors capture the semantic properties of the input texts.\n",
    "\n",
    "**Why specifically BGE embeddings?**\n",
    "\n",
    "- **High Semantic Fidelity**: BGE embeddings are designed to capture deep semantic meanings, enabling more effective document similarity checks and retrieval.\n",
    "- **Performance**: They are optimized for both accuracy and speed in large-scale applications.\n",
    "- **Versatility**: Suitable for diverse NLP tasks such as classification, clustering, and information retrieval.\n",
    "- **Accessibility through Langchain**: The Langchain library provides a simple interface to interact with the BGE embeddings, making it easier to integrate them into the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "id": "af0a4a16684d3b86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en\",\n",
    "    model_kwargs={\"device\": \"cpu\" if not USE_GPU else \"cuda:0\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chunking\n",
    "\n",
    "Another important step in the indexing process is to split the documents into smaller chunks. This division is crucial for managing large documents, improving the granularity of search results, and enhancing the efficiency of the embedding process.\n",
    "\n",
    "**Purpose and Benefits**:\n",
    "\n",
    "- **Manageability**: Smaller chunks are easier to process and store, reducing the computational burden on the system.\n",
    "- **Precision**: By embedding smaller sections of text, the system can achieve more precise document retrieval, particularly for queries that target specific information within a large document.\n",
    "- **Performance**: Smaller text chunks lead to faster processing times during both embedding and retrieval phases.\n",
    "\n",
    "For the baseline pipeline, we will use the Recursive Character Text Splitter from the Langchain library. This splitter divides the text into smaller chunks based on the character count, ensuring that each chunk is of a manageable size for processing and embedding. The following parameters are used for the splitter:\n",
    "\n",
    "- **Chunk Size**: The maximum number of characters in each chunk.\n",
    "    - We use a chunk size of 1000 characters to ensure that the chunks are small enough for efficient processing but large enough to capture meaningful information. \n",
    "- **Chunk Overlap**: The number of characters that overlap between adjacent chunks.\n",
    "    - We set the overlap to 0 to avoid redundancy and ensure that each chunk contains unique information.   \n",
    "- **Length Function**: The function used to calculate the length of the text.\n",
    "- **Is Separator Regex**: A flag indicating whether the separator is a regular expression."
   ],
   "id": "b314899e360b2d15"
  },
  {
   "cell_type": "code",
   "id": "e1a4d62c3aaa7895",
   "metadata": {},
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The helper function `create_documents` is used to create the documents from the DataFrame. The function takes the DataFrame and the text splitter as input and returns the documents. The function also prints the number of documents created and the percentage of documents created compared to the number of rows in the source DataFrame.",
   "id": "12bd888d346ed1a9"
  },
  {
   "cell_type": "code",
   "id": "f066402773ac9d58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def create_documents(df: pd.DataFrame, text_splitter, verbose=True):\n",
    "    metadata_cols = ['url', 'domain', 'title', 'author', 'date', 'id']\n",
    "    if not all(col in df.columns for col in metadata_cols + ['content']):\n",
    "        raise ValueError(\n",
    "            f\"DataFrame must contain all metadata columns and a 'content' column: {metadata_cols + ['content']}\")\n",
    "\n",
    "    metadata = df[metadata_cols].rename(columns={'id': 'origin_doc_id'}).to_dict('records')\n",
    "    docs = text_splitter.create_documents(df['content'], metadata)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"{text_splitter.__class__.__name__}: \"\n",
    "            f\"Number of documents created: {len(docs)}, \"\n",
    "            f\"Number of rows in source df: {len(df)}, \"\n",
    "            f\"Percentage of documents created: {len(docs) / len(df) * 100:.2f}%\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "documents = create_documents(df, recursive_text_splitter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "492aea9c0285cf23",
   "metadata": {},
   "source": [
    "documents[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once split, each text chunk is processed through the BGE embeddings model to generate individual vectors. These vectors are then stored in the vector store. Handling multiple embeddings per document involves indexing each chunk separately while maintaining a reference to the original document.",
   "id": "3aad96d865bc777"
  },
  {
   "cell_type": "markdown",
   "id": "ec5d15f4",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "\n",
    "We will use [ChromaDB](https://www.trychroma.com/) to store the embeddings. For easier interaction with the embeddings, we will use the `VectorStore` class which can be found in the `src/vector_store.py` file. The `VectorStore` class provides an interface that combines a specific collection of embeddings with the Langchain VectorStore API. The class allows for easy interaction with the embeddings, including adding documents and performing similarity searches.\n",
    "\n",
    "The embedding of the chunks takes time and resources, so we will use a persistent store to save the embeddings and avoid recomputing them when running the notebook multiple times. A database file with the precomputed embeddings can be downloaded and integrated according to the instructions in the README file.\n",
    "\n",
    "The collection name for the vector store is generated based on the embeddings model and the text splitter used. The collection name is used to identify the specific collection of embeddings in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "id": "f738ace88dd27a69",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from src.vector_store import VectorStore\n",
    "\n",
    "\n",
    "def get_vector_store_collection_name(embeddings, text_splitter):\n",
    "    return f\"cleantech-{embeddings.model_name}-{text_splitter.__class__.__name__}_{'dev' if DEV_MODE else 'prod'}\".lower().replace(\n",
    "        \" \", \"_\").replace(\"/\", \"_\").replace(\":\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "bge_vector_store = VectorStore(embedding_function=bge_embeddings,\n",
    "                               collection=get_vector_store_collection_name(bge_embeddings, recursive_text_splitter))\n",
    "\n",
    "get_vector_store_collection_name(bge_embeddings, recursive_text_splitter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f20213c70591f14",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "In the next step we will add the prepared documents from the previous step to the VectorStore. If the collection already exists in the VectorStore, this step is skipped."
  },
  {
   "cell_type": "code",
   "id": "62d558939648a829",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "bge_vector_store.add_documents(documents, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2177cd39686a8db",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "After adding the documents to the vector store we can now perform similarity searches on the documents to verify that the interaction with the vector store works as expected."
   ]
  },
  {
   "cell_type": "code",
   "id": "91a13de926826f7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "bge_vector_store.similarity_search_w_scores(\"The company is also aiming to reduce gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The vector store returns a list of documents with their respective similarity scores. The documents are sorted by their similarity scores in descending order. The similarity scores indicate how similar the documents are to the query text. The higher the similarity score, the more similar the document is to the query text.",
   "id": "6e0856fdcbb28167"
  },
  {
   "cell_type": "markdown",
   "id": "7ad87a39c366b0b3",
   "metadata": {},
   "source": [
    "# Baseline Pipeline\n",
    "\n",
    "The baseline pipeline is a first simple implementation of the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7bdd2bf51dc298ce",
   "metadata": {},
   "source": [
    "from src.generation import get_llm_model, LLMModel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "retriever = bge_vector_store.get_retriever()\n",
    "azure_model = get_llm_model(LLMModel.GPT_3_AZURE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcc6ebdf4dab2924",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "base_rag_prompt = \"\"\"\n",
    "Answer the question to your best knowledge when looking at the following context:\n",
    "{context}\n",
    "                \n",
    "Question: {question}\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83dfe841ed24a7a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "base_rag_chain = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | ChatPromptTemplate.from_template(base_rag_prompt)\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "base_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8b1225784152974",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "base_rag.invoke(\"Is the company aiming to reduce gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a0c9f4f51cdcb2f",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In order to compare the performance of different pipelines we need to evaluate them. The evaluation is done with the `ragas` library. The library provides a function to evaluate the performance of the pipeline. `ragas` provides predefined metrics for the evaluation which are described in the [documentation](https://docs.ragas.io/en/stable/concepts/metrics/index.html). We will use the following metrics to evaluate the performance of our pipelines:\n",
    "\n",
    "- **Context Relevancy**: The context relevancy metric measures how well the generated response is related to the context. The metric is calculated as the cosine similarity between the context and the generated response.\n",
    "\n",
    "## Evaluation Set\n",
    "In order to provide a fair comparison between the different pipelines we will use the same evaluation set for all pipelines. The evaluation set was created before hand with the script `scripts/generate_testset.py`. With that we can evaluate the performance of our pipelines with a subset of the data which saves time and resources."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad1d73f0cad6c2a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df_eval = pd.read_csv(paths['df_eval_path'])\n",
    "df_eval.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea1247f182f35d3a",
   "metadata": {},
   "source": [
    "from src.preprocessing import EvaluationPreprocessor\n",
    "\n",
    "if os.path.exists(paths['df_eval_preprocessed_path']) and USE_CACHE:\n",
    "    df_eval = pd.read_csv(paths['df_eval_preprocessed_path'])\n",
    "else:\n",
    "    df_eval = EvaluationPreprocessor(df, df_eval).preprocess()\n",
    "    df_eval.to_csv(paths['df_eval_preprocessed_path'], index=False)\n",
    "\n",
    "df_eval.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ae15782",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "The Evaluator evaluation class is a wrapper around the `ragas` library. It provides a simple interface to evaluate the performance of the pipelines. The class provides a method to evaluate the performance of the pipeline and returns the results as a pandas DataFrame. The metrics are calculated for each example in the evaluation set and results can be aggregated over the whole evaluation set to get an overall performance of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5bccbe256adad5d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from src.evaluation import Evaluator\n",
    "\n",
    "base_evaluator = Evaluator(name=\"Baseline\",\n",
    "                           cache_results=USE_CACHE,\n",
    "                           rag_chain=base_rag,\n",
    "                           llm_model=azure_model,\n",
    "                           embeddings=bge_embeddings)\n",
    "\n",
    "base_evaluator.evaluate(df_eval)\n",
    "base_evaluator.summarize()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8641b16",
   "metadata": {},
   "source": [
    "# Experiment 1: Looking at the impact of context and its chunking strategy\n",
    "\n",
    "Contrary to the apparent structure of the data, which seems to have already chunked the data according to recursive character chunking strategy, this step will introduce Semantic Chunking. This will help us to see if it is beneficial for the LLM to consider the context in a more structured manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d18816",
   "metadata": {},
   "source": [
    "In order to embed the processed documents we again can turn them into langchain-digestible Documents."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5ecdc9c7acb1455",
   "metadata": {},
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    bge_embeddings, breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "\n",
    "semantic_documents = create_documents(df, semantic_chunker)\n",
    "semantic_documents[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae6b21b4f45263c7",
   "metadata": {},
   "source": [
    "semantic_documents[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4edca8df",
   "metadata": {},
   "source": [
    "And in order to look at this experiment in an encapsulated manner, a new `VectorStore` will be created with a separate collection to keep concerns separated."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad572672",
   "metadata": {},
   "source": [
    "bge_semantic_vector_store = VectorStore(embedding_function=bge_embeddings,\n",
    "                                        collection=get_vector_store_collection_name(bge_embeddings, semantic_chunker))\n",
    "\n",
    "semantic_retriever = bge_semantic_vector_store.get_retriever()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15eb78af",
   "metadata": {},
   "source": [
    "bge_semantic_vector_store.add_documents(semantic_documents, verbose=True, batch_size=128)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8cac4a3",
   "metadata": {},
   "source": [
    "bge_semantic_vector_store.similarity_search_w_scores(\"The company is also aiming to reduce gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03a1d3e3",
   "metadata": {},
   "source": [
    "semantic_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": semantic_retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f76fca6",
   "metadata": {},
   "source": [
    "semantic_rag.invoke(\"Is the company aiming to reduce gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40e01e49",
   "metadata": {},
   "source": [
    "semantic_evaluator = Evaluator(name=\"Semantic Chunking\",\n",
    "                               cache_results=USE_CACHE,\n",
    "                               rag_chain=semantic_rag,\n",
    "                               llm_model=azure_model,\n",
    "                               embeddings=bge_embeddings)\n",
    "\n",
    "semantic_evaluator.evaluate(df_eval)\n",
    "semantic_evaluator.summarize()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4206849bf946549",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Experiment 2: Using a Multi-Query Retrieval Strategy\n",
    "\n",
    "At the heart of the RAG is the retriever, which is responsible for finding the most relevant documents for a given question. The baseline RAG uses the vector retriever to find the most relevant document, using cosine-similarity. \n",
    "\n",
    "We will now experiment with a multi-query retrieval strategy. The idea is to use multiple queries to retrieve a multidude of documents and take a unique union of the results. This way we can increase the diversity of the documents and potentially improve the quality of the generated answer. \n",
    "\n",
    "For this we will use the MultiQueryRetriever from langchain.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "569145c02e8b6f23",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "mqr_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=azure_model\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eeeaa115ecd44569",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## using the langchain template for the prompt\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "        prompt_perspectives\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6515b7c5f45f3e2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def get_unique_union(union_docs: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in union_docs for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "mqr_retrieval_chain = (\n",
    "        generate_queries\n",
    "        | mqr_retriever.map()\n",
    "        | get_unique_union\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ea2b6ac1c956550",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "mqr_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": mqr_retrieval_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=base_rag_chain)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eaece981dfdcbbd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "mqr_rag.invoke(\"Is the company aiming to reduce gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0cc9986273960c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "mqr_evaluator = Evaluator(name=\"Multi-Query Retrieval\",\n",
    "                          cache_results=USE_CACHE,\n",
    "                          rag_chain=mqr_rag,\n",
    "                          llm_model=azure_model,\n",
    "                          embeddings=bge_embeddings)\n",
    "\n",
    "mqr_evaluator.evaluate(df_eval)\n",
    "mqr_evaluator.summarize()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mqr_evaluator.eval_results",
   "id": "aba9b0bf23e95934",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc174993270d2626",
   "metadata": {},
   "source": [
    "# Experiment 3: Using a Step Back Strategy"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ec7b0badfb52e9c",
   "metadata": {},
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "generate_queries_step_back = (prompt | azure_model | StrOutputParser())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30ad54af51151f7c",
   "metadata": {},
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "response_prompt_template = \"\"\"\n",
    "You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "step_back_retrieval_chain = (\n",
    "        {\n",
    "            \"normal_context\": RunnableLambda(lambda x: x) | retriever,\n",
    "            \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        }\n",
    ")\n",
    "\n",
    "step_back_answer_chain = (\n",
    "        RunnablePassthrough.assign(normal_context=(lambda x: format_docs(x['context'][\"normal_context\"])),\n",
    "                                   step_back_context=(lambda x: format_docs(x['context'][\"step_back_context\"])),\n",
    "                                   question=(lambda x: x[\"question\"]))\n",
    "        | response_prompt\n",
    "        | azure_model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "step_back_rag = RunnableParallel(\n",
    "    {\n",
    "        \"context\": step_back_retrieval_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=step_back_answer_chain)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "step_back_rag.invoke(\"What is the company's goal in reducing gas flaring?\")",
   "id": "32170d6f70b9df78",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea96e9bf79fe6cbb",
   "metadata": {},
   "source": [
    "step_back_evaluator = Evaluator(name=\"Step Back\",\n",
    "                                cache_results=USE_CACHE,\n",
    "                                rag_chain=step_back_rag,\n",
    "                                llm_model=azure_model,\n",
    "                                embeddings=bge_embeddings)\n",
    "\n",
    "step_back_evaluator.evaluate(df_eval)\n",
    "step_back_evaluator.summarize()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ea2d12a16f7542c",
   "metadata": {},
   "source": [
    "# Experiment 4: HyDE approach"
   ]
  },
  {
   "cell_type": "code",
   "id": "80663bc00c155ead",
   "metadata": {},
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system = \"\"\"You are an expert about the Clean Technology Sector.\n",
    "            Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question.\"\"\"\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_hypothetical_doc = hyde_prompt | azure_model | StrOutputParser() | RunnablePassthrough()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb68b566d139621",
   "metadata": {},
   "source": [
    "hyde_retrieval_chain = (gen_hypothetical_doc\n",
    "            | base_rag\n",
    "            )\n",
    "\n",
    "hyde_rag = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=hyde_retrieval_chain)\n",
    "\n",
    "\n",
    "hyde_rag.invoke(\"What is the company's goal in reducing gas flaring?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4597fc4f935fc225",
   "metadata": {},
   "source": [
    "hyde_evaluator = Evaluator(name=\"HyDE\",\n",
    "                           cache_results=USE_CACHE,\n",
    "                           rag_chain=step_back_rag,\n",
    "                           llm_model=azure_model,\n",
    "                           embeddings=bge_embeddings)\n",
    "\n",
    "hyde_evaluator.evaluate(df_eval)\n",
    "hyde_evaluator.summarize()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
